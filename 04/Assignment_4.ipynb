{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neuron computes a linear function (z = Wx + b) followed by an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use linear functions, than we do not need multi-layer neural networks, we only need one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is meant to resolve classification problems where given an element you have to classify the same in N categories. Typical examples are for example given a mail to classify it as spam or not, or given a vehicle find to wich category it belongs (car, truck, van, etc ..). That's basically the output is a finite set of descrete values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend C. Because A,B and D are more suitable for inter layers. The output of sigmoid function is between 0 and 1. It represents the probablity that the data belongs to certain class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that way, everyone neuron has the same value and gradients. Both forward and backword propagation looks the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./networks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/lucca/D:/nlp/lec4\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1./(1 + np.exp(-1 * z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.randn(dim, 1)\n",
    "    b = 0\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    "\n",
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    X = normalization(X)\n",
    "\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "#     sum=0.0\n",
    "#     for x in map(lambda y,p:(1-y)*math.log(1-p)+y*math.log(p),Y,A):\n",
    "#         sum+=x\n",
    "    cost = -1/m*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n",
    "    \n",
    "    dw = np.dot(X,(A-Y).T)/m\n",
    "    db = 1/m*np.sum(A-Y)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w + (-1 * dw) * learning_rate\n",
    "        b = b + (-1 * db) * learning_rate\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "#     Y_prediction = []\n",
    "#     for i in range(A.shape[1]):\n",
    "#         Y_prediction.append([0 if A[0,i]<0.5 else 1])\n",
    "    for i in range(A.shape[1]):\n",
    "        if(A[0,i]<=0.5):\n",
    "            Y_prediction[0,i]=0\n",
    "        else:\n",
    "            Y_prediction[0,i]=1\n",
    "            \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, num_iterations, learning_rate,print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    w,b = initialize_parameters(X_train.shape[0])\n",
    "    params, grads, cost = optimize(w, b, X_train, y_train, num_iterations, learning_rate, print_cost)\n",
    "    Y_prediction_training = predict(params[\"w\"],params[\"b\"], X_train)\n",
    "    Y_prediction_test = predict(params[\"w\"],params[\"b\"], X_test)\n",
    "    traing_accuracy = 100 - np.mean(np.abs(Y_prediction_training - y_train)) * 100\n",
    "    test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - y_test)) * 100\n",
    "    d = {\"w\":w,\n",
    "         \"b\":b,\n",
    "         \"training_accuracy\": traing_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"cost\":cost}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.452802\n",
      "Cost after iteration 100: 1.298416\n",
      "Cost after iteration 200: 1.228319\n",
      "Cost after iteration 300: 1.165001\n",
      "Cost after iteration 400: 1.106163\n",
      "Cost after iteration 500: 1.051665\n",
      "Cost after iteration 600: 1.001371\n",
      "Cost after iteration 700: 0.955104\n",
      "Cost after iteration 800: 0.912655\n",
      "Cost after iteration 900: 0.873790\n",
      "Cost after iteration 1000: 0.838258\n",
      "Cost after iteration 1100: 0.805800\n",
      "Cost after iteration 1200: 0.776156\n",
      "Cost after iteration 1300: 0.749074\n",
      "Cost after iteration 1400: 0.724313\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1\n",
    "\n",
    "\n",
    "d = model(X_train.T, y_train.T, X_test.T, y_test.T, 1500, 0.01,print_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.28210838901262"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"training_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.11111111111111"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"test_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is: 0.01\n",
      "training accuracy: 65.62731997030438\n",
      "test accuracy: 65.77777777777777\n",
      "cost: 0.6835502418614197\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 0.001\n",
      "training accuracy: 44.61766889383816\n",
      "test accuracy: 44.666666666666664\n",
      "cost: 1.2331710925833332\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 0.0001\n",
      "training accuracy: 48.70081662954714\n",
      "test accuracy: 49.33333333333333\n",
      "cost: 1.5536714217947771\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: {}\".format(i))\n",
    "    models[str(i)] = model(X_train.T, y_train.T, X_test.T, y_test.T, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print(\"training accuracy: {}\".format(models[str(i)][\"training_accuracy\"]))\n",
    "    print(\"test accuracy: {}\".format(models[str(i)][\"test_accuracy\"]))\n",
    "    print(\"cost: {}\".format(models[str(i)][\"cost\"][-1]))\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is: 500\n",
      "training accuracy: 37.787676317743134\n",
      "test accuracy: 37.77777777777778\n",
      "cost: 1.2842360453833843\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 1500\n",
      "training accuracy: 64.5879732739421\n",
      "test accuracy: 62.666666666666664\n",
      "cost: 0.7072650319525251\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 3000\n",
      "training accuracy: 77.87676317743133\n",
      "test accuracy: 78.88888888888889\n",
      "cost: 0.492453486329891\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_iterations = [500, 1500, 3000]\n",
    "models = {}\n",
    "for i in num_iterations:\n",
    "    print (\"learning rate is: {}\".format(i))\n",
    "    models[str(i)] = model(X_train.T, y_train.T, X_test.T, y_test.T, num_iterations = i, learning_rate = 0.01, print_cost = False)\n",
    "    print(\"training accuracy: {}\".format(models[str(i)][\"training_accuracy\"]))\n",
    "    print(\"test accuracy: {}\".format(models[str(i)][\"test_accuracy\"]))\n",
    "    print(\"cost: {}\".format(models[str(i)][\"cost\"][-1]))\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digit (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 6.473211\n",
      "Cost after iteration 500: 4.188889\n",
      "Cost after iteration 1000: 3.510824\n",
      "Cost after iteration 1500: 2.979681\n",
      "Cost after iteration 2000: 2.556976\n",
      "Cost after iteration 2500: 2.223167\n",
      "Cost after iteration 3000: 1.958529\n",
      "Cost after iteration 3500: 1.746743\n",
      "Cost after iteration 4000: 1.575379\n",
      "Cost after iteration 4500: 1.435151\n",
      "Cost after iteration 5000: 1.319117\n",
      "Cost after iteration 5500: 1.222053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| Y_prediction_test: array([6, 5, 9, 4, 7, 1, 4, 5, 2, 2, 1, 8, 8, 8, 8, 9, 0, 0, 0, 6, 2, 0,\n",
      "                              2, 3, 6, 9, 5, 8, 5, 6, 9, 8, 7, 4, 6, 3, 6, 5, 3, 4, 6, 3, 0, 6,\n",
      "                              4, 0, 0, 0, 9, 7, 9, 8, 2, 7, 1, 6, 0, 4, 8, 6, 1, 7, 1, 4, 0, 5,\n",
      "                              5, 6, 1, 7, 5, 5, 1, 3, 0, 9, 0, 9, 5, 3, 5, 9, 6, 4, 8, 4, 7, 9,\n",
      "                              0, 7, 2, 0, 3, 8, 8, 2, 7, 9, 6, 7, 1, 5, 4, 3, 1, 4, 8, 3, 0, 6,\n",
      "                              0, 3, 5, 5, 4, 5, 2, 1, 6, 1, 3, 1, 9, 5, 0, 3, 5, 2, 5, 9, 4, 1,\n",
      "                              5, 8, 5, 4, 7, 4, 6, 6, 4, 2, 4, 1, 3, 5, 5, 7, 1, 3, 5, 1, 0, 6,\n",
      "                              6, 6, 7, 0, 3, 0, 8, 6, 1, 7, 3, 9, 9, 8, 8, 3, 2, 3, 2, 4, 6, 4,\n",
      "                              2, 1, 4, 5, 4, 5, 8, 3, 1, 1, 3, 5, 2, 6, 3, 2, 1, 5, 6, 0, 0, 8,\n",
      "                              4, 8, 1, 0, 7, 0, 7, 1, 7, 1, 2, 8, 6, 0, 6, 5, 3, 7, 7, 0, 4, 6,\n",
      "                              0, 6, 8, 9, 1, 1, 8, 1, 7, 9, 1, 1, 7, 5, 0, 3, 0, 2, 0, 1, 4, 7,\n",
      "                              3, 2, 0, 7, 0, 2, 1, 1, 6, 2, 7, 8, 8, 2, 2, 4, 3, 7, 1, 2, 2, 5,\n",
      "                              1, 7, 1, 8, 8, 6, 7, 0, 2, 7, 4, 4, 5, 1, 9, 7, 6, 1, 1, 9, 6, 0,\n",
      "                              9, 5, 0, 8, 8, 4, 9, 3, 7, 8, 9, 4, 6, 3, 2, 7, 2, 2, 5, 5, 2, 8,\n",
      "                              5, 4, 2, 4, 7, 0, 6, 2, 9, 6, 7, 1, 8, 9, 8, 5, 0, 4, 2, 6, 8, 5,\n",
      "                              0, 7, 6, 1, 3, 0, 8, 6, 8, 3, 5, 7, 3, 0, 4, 2, 5, 7, 9, 4, 3, 4,\n",
      "                              3, 6, 7, 3, 8, 2, 4, 4, 1, 3, 8, 8, 5, 4, 6, 6, 4, 7, 2, 8, 9, 0,\n",
      "                              6, 4, 1, 7, 4, 7, 1, 7, 1, 3, 1, 9, 9, 6, 6, 0, 3, 4, 8, 8, 4, 5,\n",
      "                              3, 7, 4, 4, 3, 3, 8, 7, 7, 4, 4, 1, 9, 6, 7, 3, 5, 0, 6, 4, 4, 4,\n",
      "                              3, 9, 9, 8, 6, 6, 6, 8, 9, 0, 5, 5, 3, 1, 4, 7, 1, 4, 6, 5, 9, 9,\n",
      "                              6, 6, 1, 9, 3, 4, 6, 1, 1, 1])\n",
      "ic| y_test_target: array([2, 5, 9, 9, 7, 8, 9, 5, 2, 2, 1, 8, 1, 8, 8, 9, 0, 0, 0, 6, 2, 4,\n",
      "                          2, 3, 6, 5, 5, 8, 5, 6, 3, 8, 7, 4, 6, 3, 6, 5, 3, 4, 6, 2, 0, 6,\n",
      "                          4, 0, 0, 0, 7, 7, 9, 1, 2, 7, 2, 6, 0, 4, 3, 6, 1, 7, 1, 4, 9, 5,\n",
      "                          5, 6, 8, 7, 5, 5, 6, 2, 0, 9, 0, 9, 5, 3, 5, 9, 6, 4, 8, 4, 7, 9,\n",
      "                          9, 0, 2, 9, 3, 8, 8, 2, 7, 9, 6, 9, 8, 5, 4, 2, 1, 4, 8, 3, 0, 6,\n",
      "                          0, 3, 5, 5, 4, 5, 2, 1, 6, 1, 8, 4, 9, 5, 0, 3, 5, 8, 5, 5, 4, 1,\n",
      "                          0, 8, 5, 4, 7, 4, 6, 2, 4, 3, 4, 1, 3, 5, 6, 7, 2, 1, 5, 1, 0, 6,\n",
      "                          6, 6, 7, 0, 3, 0, 3, 6, 6, 7, 1, 9, 9, 8, 8, 3, 2, 3, 2, 4, 6, 4,\n",
      "                          2, 8, 4, 5, 4, 8, 8, 5, 1, 8, 5, 0, 3, 6, 3, 2, 1, 5, 6, 0, 0, 3,\n",
      "                          4, 8, 1, 0, 7, 0, 7, 1, 7, 1, 2, 3, 2, 0, 6, 5, 3, 7, 9, 0, 7, 6,\n",
      "                          0, 6, 3, 9, 4, 1, 1, 1, 7, 9, 2, 3, 7, 5, 0, 3, 0, 3, 2, 1, 9, 7,\n",
      "                          3, 2, 0, 7, 0, 2, 1, 1, 8, 2, 7, 7, 5, 3, 3, 4, 8, 7, 1, 2, 2, 5,\n",
      "                          4, 7, 1, 3, 8, 6, 7, 0, 2, 7, 4, 4, 5, 8, 9, 7, 6, 1, 1, 9, 6, 0,\n",
      "                          9, 8, 0, 7, 3, 4, 5, 3, 7, 8, 4, 9, 6, 3, 3, 7, 2, 2, 5, 9, 2, 8,\n",
      "                          5, 4, 2, 4, 7, 0, 1, 2, 9, 6, 7, 2, 8, 4, 8, 5, 0, 4, 2, 6, 8, 5,\n",
      "                          0, 7, 6, 1, 2, 0, 8, 6, 8, 9, 5, 7, 5, 0, 4, 3, 5, 3, 9, 4, 5, 4,\n",
      "                          3, 6, 7, 3, 8, 1, 9, 4, 1, 3, 3, 8, 5, 1, 6, 8, 4, 7, 2, 8, 5, 0,\n",
      "                          6, 4, 1, 9, 4, 7, 1, 7, 1, 3, 1, 9, 9, 6, 6, 0, 3, 4, 8, 8, 4, 5,\n",
      "                          3, 9, 4, 7, 9, 3, 8, 7, 4, 4, 4, 1, 4, 6, 7, 3, 5, 0, 6, 4, 4, 4,\n",
      "                          5, 9, 9, 8, 6, 6, 6, 8, 4, 0, 5, 5, 9, 1, 4, 7, 1, 7, 6, 0, 9, 1,\n",
      "                          6, 6, 8, 8, 8, 0, 6, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "\n",
    "# Loading the data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Split the data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n",
    "# print(y_test[1])\n",
    "y_test_multi =np.zeros((y_test.shape[0],10))\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_multi[i,y_test[i]]= 1\n",
    "\n",
    "y_train_multi =np.zeros((y_train.shape[0],10))\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_train_multi[i,y_train[i]]= 1\n",
    "\n",
    "\n",
    "# reformulate the label.\n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "def softmax(X):  # softmax函数\n",
    "    ans =np.zeros((X.shape[0],X.shape[1]))\n",
    "    for i in range(X.shape[1]):\n",
    "        ans[:,i] = np.exp(X[:,i]) / np.sum(np.exp(X[:,i]))\n",
    "    return ans\n",
    "\n",
    "# 觉得这种写法比较费事，不知道助教有没有好的建议啊\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "\n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "\n",
    "    w = np.random.randn(dim, 10)\n",
    "    b = 0\n",
    "\n",
    "    assert (w.shape == (dim, 10))\n",
    "    assert (isinstance(b, float) or isinstance(b, int))\n",
    "\n",
    "    return w, b\n",
    "\n",
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    "\n",
    "def propagate_1(w, b, X, Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "\n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    X = normalization(X)\n",
    "\n",
    "    m = X.shape[1]\n",
    "    A = softmax(np.dot(w.T, X) + b)\n",
    "    summ = np.sum(A[:,1])\n",
    "    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "\n",
    "    dw = np.dot(X, (A - Y).T) / m\n",
    "    db = 1 / m * np.sum(A - Y)\n",
    "\n",
    "    assert (dw.shape == w.shape)\n",
    "    assert (db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert (cost.shape == ())\n",
    "\n",
    "    grads = {'dw': dw,\n",
    "             'db': db}\n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "\n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "\n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "    '''\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        grads, cost = propagate_1(w, b, X, Y)\n",
    "\n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "\n",
    "        w = w + (-1 * dw) * learning_rate\n",
    "        b = b + (-1 * db) * learning_rate\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 500 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias\n",
    "    X -- data\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 10)\n",
    "\n",
    "    A = softmax(np.dot(w.T, X) + b)\n",
    "\n",
    "    #     Y_prediction = []\n",
    "    #     for i in range(A.shape[1]):\n",
    "    #         Y_prediction.append([0 if A[0,i]<0.5 else 1])\n",
    "    # for i in range(A.shape[1]):\n",
    "    #     if (A[0, i] <= 0.5):\n",
    "    #         Y_prediction[0, i] = 0\n",
    "    #     else:\n",
    "    #         Y_prediction[0, i] = 1\n",
    "    Y_prediction = A\n",
    "    assert (Y_prediction.shape == (10, m))\n",
    "\n",
    "    return Y_prediction\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    accuracy_num = 0\n",
    "    for i in range(y_hat.shape[0]):\n",
    "        if y_hat[i]==y[i]:\n",
    "            accuracy_num += 1\n",
    "    accuracy_num /= y_hat.shape[0]\n",
    "    return  accuracy_num\n",
    "\n",
    "def model(X_train, y_train, X_test, y_test, num_iterations, learning_rate, y_train_target, y_test_target, print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    w, b = initialize_parameters(X_train.shape[0])\n",
    "    params, grads, cost = optimize(w, b, X_train, y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    Y_prediction_training = np.argmax(predict(params[\"w\"], params[\"b\"], X_train),axis=0)\n",
    "    Y_prediction_test = np.argmax(predict(params[\"w\"], params[\"b\"], X_test),axis=0)\n",
    "    ic(Y_prediction_test)\n",
    "    ic(y_test_target)\n",
    "\n",
    "    training_accuracy = accuracy(Y_prediction_training, y_train_target)\n",
    "    test_accuracy = accuracy(Y_prediction_test, y_test_target)\n",
    "    d = {\"w\": w,\n",
    "         \"b\": b,\n",
    "         \"training_accuracy\": training_accuracy,\n",
    "         \"test_accuracy\": test_accuracy,\n",
    "         \"cost\": cost}\n",
    "    return d\n",
    "\n",
    "d = model(X_train.T, y_train_multi.T, X_test.T, y_test_multi.T, 6000, 0.006, y_train.T, y_test.T, print_cost=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7787676317743133\n",
      "0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(d[\"training_accuracy\"])\n",
    "print(d[\"test_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
