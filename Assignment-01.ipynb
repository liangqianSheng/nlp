{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2020年1月05日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 作业截止时间\n",
    "此次作业截止时间为 2020.01.12日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：每道题是否回答完整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Image recognition, automated geophysical feature detection, object classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: First we can create branches and perform operations, then we can commit files, merge changes and clone the files.\n",
    "Jupyter is an open-source web application that allows us to create and share documents that contain live code and explanatory test.\n",
    "Pycharm is an IDE. It is used for development in Python.The best thing about PyCharm is that it bundles with all the features which is required to develop application in Python language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: A probability model is a mathematical representation of a random phenomenon. It is defined by its sample space, events within the sample space, and probabilities associated with each event. The sample space S for a probability model is the set of all possible outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Medical image processing, quantitative futures forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:It is of fundamental importance in scientific reasoning, and is widely used to model uncertainty and random processes in scientific and engineering applications.\n",
    "Programming based on parsing and pattern match is context dependent, so it has to be changed as long as the content is changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans： A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability. to the whole sequence. The language model provides context to distinguish between words and phrases that sound similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Chat robots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1)–order Markov model. 1-gram model is a specific case of n-gram language model, and it just uses the word itself to predict next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Advantages: Models are not biased by hand coded lists of words, but are completely dependent on real data\n",
    "Disadvantages: Long range dependencies are not captured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: A probabilistic language model for predicting the next item using one previous word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "you_need_replace_this_with_name_you_given = '''\n",
    "# you code here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_1 = '''\n",
    "daily_life = someone do_somethings on one_day\n",
    "someone = My father | My sister | My mother | My classmate | My workmate\n",
    "do_somethings = eats food | reads papers | do_sports | goes to somewhere\n",
    "food = sandwich | noodles | steaks | eggs | bread | wheats\n",
    "papers = newspaper | novels | news | books | articles\n",
    "do_sports  = plays football | goes jogging | goes swimming\n",
    "somewhere = school | company | park | shopping malls\n",
    "one_day = weekends | Monday | Tuesday | Wednesday | Thursday | Friday\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否提出了和课程上区别较大的语法结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "you_need_replace_this_with_name_you_given = '''\n",
    "# you code here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_2 = '''\n",
    "sentence = 主语结构 谓语结构 宾语结构\n",
    "主语结构 = 定语 主语 | 主语\n",
    "谓语结构 = 状语 谓语 | 谓语\n",
    "宾语结构 = 定语 宾语 | 宾语\n",
    "定语 = 高大的 | 矮小的 | 巨大的 | 渺小的 | 漂亮的 | 丑陋的 | 红色的 | 饥饿的 | 饱满的 | 可怜的 | 快乐的 | 补水的 | 无奈的\n",
    "主语 = 我 | 你 | 他 | 她 | 它 | 我们 | 你们 | 他们\n",
    "状语 = 狠狠地 | 高兴地 | 快速地 | 奇怪地 | 绝对地\n",
    "谓语 = 跑 | 跳 | 玩耍 | 打 | 打扮 | 践踏 | 鄙视 | 尊重 | 喜欢\n",
    "宾语 = 小狗 | 女孩 | 工人 | 足球 | 电脑 | 手机 | 电话 | 北京 | 大海\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：是否和上一个语法区别比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(gram: str, split_line='\\n',split='='):\n",
    "    grammar={}\n",
    "    for line in gram.split(split_line):\n",
    "        if not line.strip(): continue\n",
    "        exp, word = line.split(split)\n",
    "        grammar[exp.strip()]=[e.split() for e in word.split('|')]\n",
    "    return grammar\n",
    "\n",
    "gram_1 = create_grammar(grammer_1)\n",
    "gram_2 = create_grammar(grammer_2)      \n",
    "import random\n",
    "def generate(gram, target, language):\n",
    "    if target not in gram:\n",
    "        return target\n",
    "    expand = [generate(gram,t, language) for t in random.choice(gram[target])]\n",
    "    if language == 'English':\n",
    "        return ' '.join([e for e in expand if e !='null'])\n",
    "    else:\n",
    "        return ''.join([e for e in expand if e !='null'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(gram, target, n,language):\n",
    "    if n <= 0: return 'n should be larger than zero'\n",
    "    sens=[]\n",
    "    for i,sen in enumerate([generate(gram,target,language) for i in range(n)]):\n",
    "        print('sentence {}: {}'.format(i+1,sen))\n",
    "        sens.append(sen)\n",
    "    return sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1: My mother eats bread on Monday\n",
      "sentence 2: My sister eats wheats on Wednesday\n",
      "sentence 3: My sister goes to park on Tuesday\n",
      "sentence 4: My classmate goes swimming on Friday\n",
      "sentence 5: My classmate reads books on weekends\n",
      "sentence 6: My sister goes to company on Wednesday\n",
      "sentence 7: My sister plays football on Tuesday\n",
      "sentence 8: My workmate eats steaks on Tuesday\n",
      "sentence 9: My mother eats wheats on Wednesday\n",
      "sentence 10: My classmate eats noodles on Tuesday\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['My mother eats bread on Monday',\n",
       " 'My sister eats wheats on Wednesday',\n",
       " 'My sister goes to park on Tuesday',\n",
       " 'My classmate goes swimming on Friday',\n",
       " 'My classmate reads books on weekends',\n",
       " 'My sister goes to company on Wednesday',\n",
       " 'My sister plays football on Tuesday',\n",
       " 'My workmate eats steaks on Tuesday',\n",
       " 'My mother eats wheats on Wednesday',\n",
       " 'My classmate eats noodles on Tuesday']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(gram_1,'daily_life',10,'English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1: 它奇怪地鄙视足球\n",
      "sentence 2: 我们玩耍大海\n",
      "sentence 3: 快乐的他们打可怜的电脑\n",
      "sentence 4: 可怜的它快速地跳女孩\n",
      "sentence 5: 红色的我们喜欢无奈的北京\n",
      "sentence 6: 他玩耍可怜的北京\n",
      "sentence 7: 漂亮的我狠狠地跑大海\n",
      "sentence 8: 我们狠狠地喜欢饱满的手机\n",
      "sentence 9: 她高兴地打扮可怜的电脑\n",
      "sentence 10: 快乐的我跳手机\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['它奇怪地鄙视足球',\n",
       " '我们玩耍大海',\n",
       " '快乐的他们打可怜的电脑',\n",
       " '可怜的它快速地跳女孩',\n",
       " '红色的我们喜欢无奈的北京',\n",
       " '他玩耍可怜的北京',\n",
       " '漂亮的我狠狠地跑大海',\n",
       " '我们狠狠地喜欢饱满的手机',\n",
       " '她高兴地打扮可怜的电脑',\n",
       " '快乐的我跳手机']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(gram_2,'sentence',10,'Chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据以上两种语法结构可以看出，还是要注意句子内部词组的搭配使用，不然句子就会很奇怪，第一种语法结构组合了相关单词，就没有那么奇怪。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**; 运行代码，观察是否能够生成多个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucca/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>chinese</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>disability-insurance</td>\n",
       "      <td>法律要求残疾保险吗？</td>\n",
       "      <td>Is  Disability  Insurance  Required  By  Law?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>life-insurance</td>\n",
       "      <td>债权人可以在死后人寿保险吗？</td>\n",
       "      <td>Can  Creditors  Take  Life  Insurance  After ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>renters-insurance</td>\n",
       "      <td>旅行者保险有租赁保险吗？</td>\n",
       "      <td>Does  Travelers  Insurance  Have  Renters  In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>auto-insurance</td>\n",
       "      <td>我可以开一辆没有保险的新车吗？</td>\n",
       "      <td>Can  I  Drive  A  New  Car  Home  Without  In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>life-insurance</td>\n",
       "      <td>人寿保险的现金转出价值是否应纳税？</td>\n",
       "      <td>Is  The  Cash  Surrender  Value  Of  Life  In...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 category              chinese  \\\n",
       "0   disability-insurance           法律要求残疾保险吗？    \n",
       "1         life-insurance       债权人可以在死后人寿保险吗？    \n",
       "2      renters-insurance         旅行者保险有租赁保险吗？    \n",
       "3         auto-insurance      我可以开一辆没有保险的新车吗？    \n",
       "4         life-insurance    人寿保险的现金转出价值是否应纳税？    \n",
       "\n",
       "                                             english  \n",
       "0      Is  Disability  Insurance  Required  By  Law?  \n",
       "1   Can  Creditors  Take  Life  Insurance  After ...  \n",
       "2   Does  Travelers  Insurance  Have  Renters  In...  \n",
       "3   Can  I  Drive  A  New  Car  Home  Without  In...  \n",
       "4   Is  The  Cash  Surrender  Value  Of  Life  In...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "filename = 'train.txt'\n",
    "import pandas as pd\n",
    "content = pd.read_table('train.txt',header=None,sep='\\+\\+\\$\\+\\+',names = ['category', 'chinese', 'english'],index_col=0)\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = content['chinese'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12889"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def token(string):\n",
    "    return re.findall('\\w',string)\n",
    "\n",
    "import jieba\n",
    "def cut(string):\n",
    "    return jieba.lcut(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_clean = [''.join(token(a)) for a in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['法律要求残疾保险吗',\n",
       " '债权人可以在死后人寿保险吗',\n",
       " '旅行者保险有租赁保险吗',\n",
       " '我可以开一辆没有保险的新车吗',\n",
       " '人寿保险的现金转出价值是否应纳税',\n",
       " '如何报告年金收入',\n",
       " 'AAA家庭保险涵盖什么',\n",
       " '什么是简单的退休计划',\n",
       " '社会保险残疾保险是什么',\n",
       " '汽车保险是否预付']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('train_clean.txt','w') as f:\n",
    "    for a in articles_clean:\n",
    "        f.write(a + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['法律', '要求', '残疾', '保险', '吗', '\\n', '债权人', '可以', '在', '死']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN =[]\n",
    "for b in (open('train_clean.txt')):\n",
    "    TOKEN += cut(b)\n",
    "    \n",
    "TOKEN[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words_count = Counter(TOKEN)\n",
    "\n",
    "def prob_1(word):\n",
    "    return words_count[word]/len(TOKEN)\n",
    "\n",
    "TOKEN_2_gram = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-1]))]\n",
    "\n",
    "words_count_2 = Counter(TOKEN_2_gram)\n",
    "\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2: return words_count_2[word1+word2] / len(TOKEN_2_gram)\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_gram)\n",
    "    \n",
    "def get_probablity(sentence):\n",
    "    words = list(cut(sentence))\n",
    "    sentence_pro = 1\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        proba = prob_2(word,words[i+1])\n",
    "        sentence_pro *= proba\n",
    "    sentence_pro *= prob_1(words[-1])\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015128576074644776"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('健康','保险')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.962211394630653e-12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('健康保险有吗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你有多少人寿保险 is more possible\n",
      "-- 你有多少健康保险 with probility 5.763822760974605e-14\n",
      "-- 你有多少人寿保险 with probility 2.862146637762585e-11\n",
      "什么是租赁保险 is more possible\n",
      "-- 租客不保险是什么 with probility 1.7629519762721942e-17\n",
      "-- 什么是租赁保险 with probility 8.418394027307592e-11\n",
      "小孩有保险吗 is more possible\n",
      "-- 小孩有保险吗 with probility 1.4026038015717785e-13\n",
      "-- 大人保险有吗 with probility 2.1958572235957395e-15\n",
      "养乐多绿来一杯 is more possible\n",
      "-- 洋葱奶昔来一杯 with probility 0.0\n",
      "-- 养乐多绿来一杯 with probility 0.0\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"你有多少健康保险 你有多少人寿保险\",\n",
    "    \"租客不保险是什么 什么是租赁保险\",\n",
    "    \"小孩有保险吗 大人保险有吗\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1,s2 = s.split()\n",
    "    p1, p2 = get_probablity(s1), get_probablity(s2)\n",
    "    \n",
    "    better = s1 if p1>p2 else s2\n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*2 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*2 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点** 1. 是否使用了新的数据集； 2. csv(txt)数据是否正确解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于前面保险文件的内容比较少，所以还是用课上的新闻数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "TOKEN2 =[]\n",
    "for i, line in enumerate((open('article_9k.txt'))):\n",
    "    if i % 10000 == 0: print(i)\n",
    "    TOKEN2 += cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prob_1(word,TOKEN_N,words_count):\n",
    "    return words_count[word]/len(TOKEN_N)\n",
    "\n",
    "\n",
    "\n",
    "def prob_2(word1, word2,TOKEN_2_gram,words_count_2):\n",
    "    if word1 + word2 in words_count_2: return words_count_2[word1+word2] / len(TOKEN_2_gram)\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_gram)\n",
    "    \n",
    "def get_probablity(sentence,TOKEN_2_gram,words_count_2,TOKEN_N,words_count):\n",
    "    words = list(cut(sentence))\n",
    "    sentence_pro = 1\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        proba = prob_2(word,words[i+1],TOKEN_2_gram,words_count_2)\n",
    "        sentence_pro *= proba\n",
    "    sentence_pro *= prob_1(words[-1],TOKEN_N,words_count)\n",
    "    return sentence_pro\n",
    "\n",
    "def generate_best(gram, target, n,language,TOKEN_N):\n",
    "    # you code here\n",
    "    generate_prob =[]\n",
    "    words_count = Counter(TOKEN_N)\n",
    "    TOKEN_2_gram = [''.join(TOKEN_N[i:i+2]) for i in range(len(TOKEN_N[:-1]))]\n",
    "    words_count_2 = Counter(TOKEN_2_gram)\n",
    "    for sen in [generate(gram, target,language) for i in range(n)]:\n",
    "        generate_prob.append((sen, get_probablity(sen,TOKEN_2_gram,words_count_2,TOKEN_N,words_count)))\n",
    "        print(generate_prob[-1])\n",
    "    generate_prob_sort = sorted(generate_prob,key=lambda x: x[1], reverse=True)\n",
    "    return generate_prob_sort[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('矮小的他们奇怪地跳电话', 4.915335164687628e-45)\n",
      "('渺小的我们奇怪地跳饱满的小狗', 7.042768983521767e-58)\n",
      "('快乐的我们玩耍北京', 4.2057218276673925e-28)\n",
      "('你玩耍高大的电话', 5.3584416731743674e-30)\n",
      "('可怜的他高兴地跑女孩', 1.492488765024006e-40)\n",
      "('你们打扮工人', 3.501710408104236e-19)\n",
      "('他们快速地喜欢北京', 9.494104719882827e-29)\n",
      "('矮小的我玩耍丑陋的足球', 5.363161346877824e-42)\n",
      "('你狠狠地喜欢无奈的北京', 1.3334596254861192e-41)\n",
      "('渺小的他打扮电脑', 1.6302018424274733e-30)\n",
      "('饥饿的他奇怪地跳饱满的足球', 1.1548913481987155e-53)\n",
      "('红色的我们奇怪地玩耍补水的电脑', 2.917932419576423e-56)\n",
      "('我们高兴地打饥饿的北京', 2.7839371364740814e-40)\n",
      "('红色的他们鄙视北京', 1.6758938923615223e-28)\n",
      "('你打矮小的小狗', 1.1197528786534371e-32)\n",
      "('红色的他们喜欢快乐的电脑', 1.9183636198525617e-39)\n",
      "('它奇怪地打小狗', 1.2830501734570633e-33)\n",
      "('他践踏小狗', 3.6205674193453724e-20)\n",
      "('红色的你绝对地跳可怜的北京', 2.5972990853492965e-52)\n",
      "('他快速地践踏补水的北京', 2.585278865738395e-41)\n",
      "('高大的他跑女孩', 1.54472148173446e-28)\n",
      "('漂亮的她打扮饥饿的手机', 7.009565636563063e-40)\n",
      "('你们绝对地尊重快乐的小狗', 1.2625964024013444e-45)\n",
      "('你高兴地打扮电脑', 3.1118384923184534e-32)\n",
      "('饥饿的它狠狠地尊重渺小的工人', 5.328702330006168e-56)\n",
      "('它尊重饱满的足球', 2.2839427685622994e-29)\n",
      "('渺小的你高兴地鄙视女孩', 3.6407337722023646e-44)\n",
      "('他鄙视工人', 3.501710408104236e-19)\n",
      "('我鄙视矮小的工人', 2.3690478753707894e-31)\n",
      "('我们跳女孩', 1.3732556221860478e-19)\n"
     ]
    }
   ],
   "source": [
    "sen_best,pro = generate_best(gram_2,'sentence',30,'chinese',TOKEN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best sentence: 你们打扮工人 with probability: 3.501710408104236e-19\n"
     ]
    }
   ],
   "source": [
    "print('best sentence: {} with probability: {}'.format(sen_best,pro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否使用 lambda 语法进行排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  The model doesn't consider the logical relations between predicate and object of the sentence. It justs randomly choose one item of predicate and object, and compute their probability. Maybe we should\n",
    " add weights to some possible pairs to make the sentence more reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**评阅点**: 是否提出了比较实际的问题，例如OOV问题，例如数据量，例如变成 3-gram问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# github 地址 https://github.com/liangqianSheng/nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
