{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "# 见另一个文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Model means target functions that maps input variables (X) to an output variable (Y). We can not find a function to fit all the data and make the right predictions. But some functions can have a better performance which can be used to solve specific predictions problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Overfitting refers to a model that models the training data too well.\n",
    "\n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.\n",
    "\n",
    "Overfitting is more likely with nonparametric and nonlinear models that have more flexibility when learning a target function. As such, many nonparametric machine learning algorithms also include parameters or techniques to limit and constrain how much detail the model learns.\n",
    "\n",
    "Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "\n",
    "Underfitting happens when a model can not learn the detail well and miss some important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Recall is the Ratio of the correct predictions and the total number of correct items in the set. It is expressed as % of the total correct(positive) items correctly predicted by the model. \n",
    "\n",
    "Precision is measured over the total predictions of the model. It is the ratio between the correct predictions and the total predictions.\n",
    "\n",
    "Unlike precision-recall curves, ROC (Receiver Operator Characteristic) curves work best for balanced data sets such as ours. Briefly, AUC is the area under the ROC curve that represents the tradeoff between Recall (TPR) and Specificity (FPR). \n",
    "\n",
    "F1，F2 score： It combines precision and recall into one metric by calculating the harmonic mean between those two.\n",
    "\n",
    "Recall is the opposite of precision, it measures false negatives against true positives. False negatives are especially important to prevent in disease detection and other predictions involving safety.\n",
    "\n",
    "F1-score is a balanced metric that appropriately quantifies the correctness of models across many domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Machine learning is using data to approximate a target function (f) that maps input variables (X) to an output variable (Y). We dont need to calculate the function by ourselves, we can converge a loss function to find the target function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation for machine learning is to evaluate the target funtion. A suitable evaluation can help us to find the suitable target function and get the expected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame.from_dict(mock_data)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('income', '-10')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from icecream import ic\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "def entropy(elements):\n",
    "    counter = Counter(elements)  ## 计算每个数字个数\n",
    "    ## 计算信息熵的概率，概率为每个数字出现个数除以数字总数\n",
    "    probs=[counter[c]/len(elements) for c in set(elements)]## set 为了不重复\n",
    "\n",
    "    return -sum(p * np.log(p) for p in probs)\n",
    "\n",
    "#下面这个函数的目的是计算所有特征中数据熵最少的\n",
    "def find_the_optimal_spilter(training_data: pd.DataFrame, target: str):\n",
    "    features = set(training_data.columns.to_list())-{target}\n",
    "    split = None\n",
    "    entropy_min = float('inf') # 求最小值，要初始化最大值\n",
    "    \n",
    "    for feature in features:\n",
    "#         ic(feature)\n",
    "        values = set(training_data[feature])\n",
    "        \n",
    "        for value in values:\n",
    "#             ic(value)\n",
    "            split_1 = training_data[training_data[feature] == value][target].tolist()\n",
    "#             ic(split_1)\n",
    "            split_2 = training_data[training_data[feature] != value][target].tolist()\n",
    "#             ic(split_2)\n",
    "            \n",
    "            entropy_add = entropy(split_1)+entropy(split_2)\n",
    "#             ic(entropy_add)\n",
    "    \n",
    "            if entropy_add<= entropy_min:\n",
    "                entropy_min = entropy_add\n",
    "                split = (feature,value)\n",
    "                \n",
    "#         ic(split)\n",
    "#         ic(entropy_min)\n",
    "    return split\n",
    "\n",
    "\n",
    "find_the_optimal_spilter(dataset, 'bought')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  bought\n",
       "2      F    +10       1\n",
       "6      M    -10       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_left = dataset[dataset['family_number'] == 2].copy()\n",
    "dataset_right = dataset[dataset['family_number'] != 2].copy()\n",
    "dataset_left.drop('family_number',axis=1,inplace=True)\n",
    "dataset_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  bought\n",
       "0      F    +10       1\n",
       "1      F    -10       1\n",
       "3      F    +10       0\n",
       "4      M    +10       0\n",
       "5      M    +10       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_right.drop('family_number',axis=1,inplace=True)\n",
    "dataset_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('income', '-10')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter(dataset_left, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_left.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not sure'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predicate(gender, income, family_number):\n",
    "    input = {}\n",
    "    input['gender']= gender\n",
    "    input['income']= income\n",
    "    input['family_number']= family_number\n",
    "    dataset_drop = dataset.copy()\n",
    "#     ic(dataset)\n",
    "    while(dataset_drop.shape[0] != 1 and dataset_drop.shape[1] != 1):\n",
    "        feature, value = find_the_optimal_spilter(dataset_drop, 'bought')\n",
    "        if input[feature]==value:\n",
    "            dataset_drop = dataset_drop[dataset_drop[feature] == value]\n",
    "            dataset_drop.drop(feature,axis=1,inplace=True)\n",
    "        else:\n",
    "            dataset_drop = dataset_drop[dataset_drop[feature] != value]\n",
    "            dataset_drop.drop(feature,axis=1,inplace=True)\n",
    "\n",
    "#     ic(dataset_drop)\n",
    "    if dataset_drop.shape[0] != 1:\n",
    "        bought = dataset_drop['bought']\n",
    "#         ic(bought)\n",
    "        \n",
    "        counter = Counter(bought)  ## 计算每个数字个数\n",
    "    ## 计算信息熵的概率，概率为每个数字出现个数除以数字总数\n",
    "        probs=[(counter[c]/len(bought),c) for c in set(bought)]## set 为了不重复\n",
    "#         ic(probs)\n",
    "        if probs[0][0]==0.5 and probs[1][0]==0.5:\n",
    "            return \"not sure\"\n",
    "        elif len(probs)==1:\n",
    "            return dataset_drop['bought'].tolist()[0]\n",
    "        else:        \n",
    "            ans = probs.sort(key=lambda x: x[0],reverse=True)[0]\n",
    "            return ans[1]\n",
    "    else: \n",
    "        return dataset_drop['bought'].tolist()[0]\n",
    "        \n",
    "    \n",
    "    \n",
    "predicate('F', '+10', 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no!, predict result:not sure, data shows:1\n",
      "yes\n",
      "yes\n",
      "no!, predict result:not sure, data shows:0\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)):\n",
    "    gender = dataset.loc[i,\"gender\"]\n",
    "    income = dataset.loc[i,\"income\"]\n",
    "    family_member = dataset.loc[i,\"family_number\"]\n",
    "    predict_result = predicate(gender,income,family_member)\n",
    "    if predict_result == dataset.loc[i,\"bought\"]:\n",
    "        print('yes')\n",
    "    else:\n",
    "        print(\"no!, predict result:{}, data shows:{}\".format(predict_result,dataset.loc[i,\"bought\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以上把表格里面所有的结果放进去重新验证了一下，除了有两个case的条件一样，但是bought有两个结果造成预测不确定以外，其余分类基本都正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| dataset:   gender income  family_number  bought\n",
      "             0      F    +10              1       1\n",
      "             1      F    -10              1       1\n",
      "             2      F    +10              2       1\n",
      "             3      F    +10              1       0\n",
      "             4      M    +10              1       0\n",
      "             5      M    +10              1       0\n",
      "             6      M    -10              2       1\n",
      "ic| feature: 'income', value: '-10'\n",
      "ic| input[feature]: '+10'\n",
      "ic| feature: 'income'\n",
      "ic| <ipython-input-27-9fd38dd5793c>:18 in predicate() at 07:15:40.162\n",
      "ic| dataset_drop:   gender income  family_number  bought\n",
      "                  0      F    +10              1       1\n",
      "                  2      F    +10              2       1\n",
      "                  3      F    +10              1       0\n",
      "                  4      M    +10              1       0\n",
      "                  5      M    +10              1       0\n",
      "ic| dataset_drop:   gender  family_number  bought\n",
      "                  0      F              1       1\n",
      "                  2      F              2       1\n",
      "                  3      F              1       0\n",
      "                  4      M              1       0\n",
      "                  5      M              1       0\n",
      "ic| feature: 'family_number', value: 2\n",
      "ic| input[feature]: 1\n",
      "ic| feature: 'family_number'\n",
      "ic| <ipython-input-27-9fd38dd5793c>:18 in predicate() at 07:15:40.239\n",
      "ic| dataset_drop:   gender  family_number  bought\n",
      "                  0      F              1       1\n",
      "                  3      F              1       0\n",
      "                  4      M              1       0\n",
      "                  5      M              1       0\n",
      "ic| dataset_drop:   gender  bought\n",
      "                  0      F       1\n",
      "                  3      F       0\n",
      "                  4      M       0\n",
      "                  5      M       0\n",
      "ic| feature: 'gender', value: 'M'\n",
      "ic| input[feature]: 'M'\n",
      "ic| feature: 'gender'\n",
      "ic| dataset_drop:    bought\n",
      "                  4       0\n",
      "                  5       0\n",
      "ic| dataset_drop:    bought\n",
      "                  4       0\n",
      "                  5       0\n",
      "ic| bought: 4    0\n",
      "            5    0\n",
      "            Name: bought, dtype: int64\n",
      "ic| probs: [(1.0, 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate('M', '+10', 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| dataset:   gender income  family_number  bought\n",
      "             0      F    +10              1       1\n",
      "             1      F    -10              1       1\n",
      "             2      F    +10              2       1\n",
      "             3      F    +10              1       0\n",
      "             4      M    +10              1       0\n",
      "             5      M    +10              1       0\n",
      "             6      M    -10              2       1\n",
      "ic| feature: 'income', value: '-10'\n",
      "ic| input[feature]: '-10'\n",
      "ic| feature: 'income'\n",
      "ic| dataset_drop:   gender  family_number  bought\n",
      "                  1      F              1       1\n",
      "                  6      M              2       1\n",
      "ic| feature: 'gender', value: 'M'\n",
      "ic| input[feature]: 'M'\n",
      "ic| feature: 'gender'\n",
      "ic| dataset_drop:    family_number  bought\n",
      "                  6              2       1\n",
      "ic| dataset_drop:    family_number  bought\n",
      "                  6              2       1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate('M', '-10', 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3346581710>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y=dataset['data'],dataset['target']\n",
    "X_rm = x[:,5]\n",
    "# plot the RM with respect to y\n",
    "plt.scatter(X_rm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assume that the target funciton is a linear function\n",
    "$$ y = k*rm + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "import random\n",
    "#define target function\n",
    "def price(rm, k, b):\n",
    "    return k * rm + b\n",
    "\n",
    "# define loss function \n",
    "def loss(y,y_hat):\n",
    "    return sum(abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define mean square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ loss = \\frac{1}{n} \\sum{|y_i - \\hat{y_i}|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ loss = \\frac{1}{n} \\sum{|y_i - (kx_i + b_i)|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define partial derivative \n",
    "def partial_derivative_k(x, y, y_hat): \n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        gradient += x_i *(1 if y_i-y_hat_i>0 else -1)\n",
    "    return -1/n * gradient\n",
    "\n",
    "def partial_derivative_b(y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "        gradient += 1 if y_i-y_hat_i>0 else -1\n",
    "    return -1 / n * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$if (y_i - \\hat{y_i} > 0):$$ \n",
    "$$ a = 1 $$\n",
    "$$ else: $$ $$ a = -1 $$\n",
    "$$ \\frac{\\partial{loss}}{\\partial{k}} = -\\frac{1}{n}\\sum a（x_i）$$\n",
    "$$ \\frac{\\partial{loss}}{\\partial{b}} = -\\frac{1}{n}\\sum(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 63.40905411071491, parameters k is 24.739349459530402 and b is -69.57137844875129\n",
      "Iteration 1, the loss is 63.005933867959634, parameters k is 24.676643866645026 and b is -69.58133892305959\n",
      "Iteration 2, the loss is 62.602813625204234, parameters k is 24.61393827375965 and b is -69.59129939736789\n",
      "Iteration 3, the loss is 62.19969338244906, parameters k is 24.551232680874275 and b is -69.60125987167619\n",
      "Iteration 4, the loss is 61.79657313969377, parameters k is 24.4885270879889 and b is -69.61122034598449\n",
      "Iteration 5, the loss is 61.39345289693856, parameters k is 24.425821495103524 and b is -69.62118082029279\n",
      "Iteration 6, the loss is 60.99033265418327, parameters k is 24.363115902218148 and b is -69.6311412946011\n",
      "Iteration 7, the loss is 60.587212411427934, parameters k is 24.300410309332772 and b is -69.6411017689094\n",
      "Iteration 8, the loss is 60.18409216867261, parameters k is 24.237704716447396 and b is -69.6510622432177\n",
      "Iteration 9, the loss is 59.780971925917434, parameters k is 24.17499912356202 and b is -69.661022717526\n",
      "Iteration 10, the loss is 59.37785168316218, parameters k is 24.112293530676645 and b is -69.6709831918343\n",
      "Iteration 11, the loss is 58.975342079202555, parameters k is 24.04958793779127 and b is -69.6809436661426\n",
      "Iteration 12, the loss is 58.57526918014035, parameters k is 23.987078787593642 and b is -69.69086461475919\n",
      "Iteration 13, the loss is 58.17667208730305, parameters k is 23.924722325143048 and b is -69.7007460376841\n",
      "Iteration 14, the loss is 57.77807499446574, parameters k is 23.862365862692453 and b is -69.710627460609\n",
      "Iteration 15, the loss is 57.37947790162837, parameters k is 23.80000940024186 and b is -69.72050888353391\n",
      "Iteration 16, the loss is 56.98088080879108, parameters k is 23.737652937791264 and b is -69.73039030645882\n",
      "Iteration 17, the loss is 56.582283715953785, parameters k is 23.67529647534067 and b is -69.74027172938372\n",
      "Iteration 18, the loss is 56.18368662311648, parameters k is 23.612940012890075 and b is -69.75015315230863\n",
      "Iteration 19, the loss is 55.78508953027918, parameters k is 23.55058355043948 and b is -69.76003457523353\n",
      "Iteration 20, the loss is 55.38649243744183, parameters k is 23.488227087988886 and b is -69.76991599815844\n",
      "Iteration 21, the loss is 54.98789534460453, parameters k is 23.42587062553829 and b is -69.77979742108334\n",
      "Iteration 22, the loss is 54.58929825176721, parameters k is 23.363514163087697 and b is -69.78967884400825\n",
      "Iteration 23, the loss is 54.190701158929926, parameters k is 23.301157700637102 and b is -69.79956026693316\n",
      "Iteration 24, the loss is 53.792104066092534, parameters k is 23.238801238186507 and b is -69.80944168985806\n",
      "Iteration 25, the loss is 53.39350697325527, parameters k is 23.176444775735913 and b is -69.81932311278297\n",
      "Iteration 26, the loss is 52.99490988041794, parameters k is 23.11408831328532 and b is -69.82920453570787\n",
      "Iteration 27, the loss is 52.596312787580636, parameters k is 23.051731850834724 and b is -69.83908595863278\n",
      "Iteration 28, the loss is 52.19771569474339, parameters k is 22.98937538838413 and b is -69.84896738155769\n",
      "Iteration 29, the loss is 51.799118601906066, parameters k is 22.927018925933535 and b is -69.85884880448259\n",
      "Iteration 30, the loss is 51.40052150906872, parameters k is 22.86466246348294 and b is -69.8687302274075\n",
      "Iteration 31, the loss is 51.00192441623142, parameters k is 22.802306001032346 and b is -69.8786116503324\n",
      "Iteration 32, the loss is 50.603327323394076, parameters k is 22.73994953858175 and b is -69.88849307325731\n",
      "Iteration 33, the loss is 50.20473023055683, parameters k is 22.677593076131156 and b is -69.89837449618221\n",
      "Iteration 34, the loss is 49.806133137719435, parameters k is 22.615236613680562 and b is -69.90825591910712\n",
      "Iteration 35, the loss is 49.4075360448822, parameters k is 22.552880151229967 and b is -69.91813734203203\n",
      "Iteration 36, the loss is 49.008938952044836, parameters k is 22.490523688779373 and b is -69.92801876495693\n",
      "Iteration 37, the loss is 48.610341859207516, parameters k is 22.428167226328778 and b is -69.93790018788184\n",
      "Iteration 38, the loss is 48.21174476637014, parameters k is 22.365810763878184 and b is -69.94778161080674\n",
      "Iteration 39, the loss is 47.81314767353292, parameters k is 22.30345430142759 and b is -69.95766303373165\n",
      "Iteration 40, the loss is 47.41455058069562, parameters k is 22.241097838976994 and b is -69.96754445665655\n",
      "Iteration 41, the loss is 47.01595348785831, parameters k is 22.1787413765264 and b is -69.97742587958146\n",
      "Iteration 42, the loss is 46.617356395021, parameters k is 22.116384914075805 and b is -69.98730730250637\n",
      "Iteration 43, the loss is 46.218759302183656, parameters k is 22.05402845162521 and b is -69.99718872543127\n",
      "Iteration 44, the loss is 45.82016220934629, parameters k is 21.991671989174616 and b is -70.00707014835618\n",
      "Iteration 45, the loss is 45.421565116509065, parameters k is 21.92931552672402 and b is -70.01695157128108\n",
      "Iteration 46, the loss is 45.022968023671766, parameters k is 21.866959064273427 and b is -70.02683299420599\n",
      "Iteration 47, the loss is 44.624370930834466, parameters k is 21.804602601822832 and b is -70.0367144171309\n",
      "Iteration 48, the loss is 44.225773837997096, parameters k is 21.742246139372238 and b is -70.0465958400558\n",
      "Iteration 49, the loss is 43.8271767451598, parameters k is 21.679889676921643 and b is -70.0564772629807\n",
      "Iteration 50, the loss is 43.4285796523225, parameters k is 21.61753321447105 and b is -70.06635868590561\n",
      "Iteration 51, the loss is 43.02998255948516, parameters k is 21.555176752020454 and b is -70.07624010883052\n",
      "Iteration 52, the loss is 42.63138546664785, parameters k is 21.49282028956986 and b is -70.08612153175542\n",
      "Iteration 53, the loss is 42.23278837381056, parameters k is 21.430463827119265 and b is -70.09600295468033\n",
      "Iteration 54, the loss is 41.83419128097325, parameters k is 21.36810736466867 and b is -70.10588437760524\n",
      "Iteration 55, the loss is 41.43559418813593, parameters k is 21.305750902218076 and b is -70.11576580053014\n",
      "Iteration 56, the loss is 41.03699709529862, parameters k is 21.24339443976748 and b is -70.12564722345505\n",
      "Iteration 57, the loss is 40.638400002461296, parameters k is 21.181037977316887 and b is -70.13552864637995\n",
      "Iteration 58, the loss is 40.23980290962398, parameters k is 21.118681514866292 and b is -70.14541006930486\n",
      "Iteration 59, the loss is 39.841205816786676, parameters k is 21.056325052415698 and b is -70.15529149222976\n",
      "Iteration 60, the loss is 39.44260872394939, parameters k is 20.993968589965103 and b is -70.16517291515467\n",
      "Iteration 61, the loss is 39.04401163111211, parameters k is 20.93161212751451 and b is -70.17505433807958\n",
      "Iteration 62, the loss is 38.645414538274764, parameters k is 20.869255665063914 and b is -70.18493576100448\n",
      "Iteration 63, the loss is 38.24681744543742, parameters k is 20.80689920261332 and b is -70.19481718392939\n",
      "Iteration 64, the loss is 37.848220352600144, parameters k is 20.744542740162725 and b is -70.2046986068543\n",
      "Iteration 65, the loss is 37.449623259762845, parameters k is 20.68218627771213 and b is -70.2145800297792\n",
      "Iteration 66, the loss is 37.051026166925496, parameters k is 20.619829815261536 and b is -70.2244614527041\n",
      "Iteration 67, the loss is 36.6524290740882, parameters k is 20.55747335281094 and b is -70.23434287562901\n",
      "Iteration 68, the loss is 36.25383198125089, parameters k is 20.495116890360347 and b is -70.24422429855392\n",
      "Iteration 69, the loss is 35.85607140306773, parameters k is 20.432760427909752 and b is -70.25410572147882\n",
      "Iteration 70, the loss is 35.46044287742412, parameters k is 20.370636178897893 and b is -70.26394761871202\n",
      "Iteration 71, the loss is 35.06496125751206, parameters k is 20.308511929886034 and b is -70.27378951594523\n",
      "Iteration 72, the loss is 34.67143987713896, parameters k is 20.24655123818643 and b is -70.28359188748672\n",
      "Iteration 73, the loss is 34.27791849676584, parameters k is 20.184590546486827 and b is -70.29339425902822\n",
      "Iteration 74, the loss is 33.884397116392776, parameters k is 20.122629854787224 and b is -70.30319663056972\n",
      "Iteration 75, the loss is 33.49087573601965, parameters k is 20.06066916308762 and b is -70.31299900211121\n",
      "Iteration 76, the loss is 33.09735435564655, parameters k is 19.998708471388017 and b is -70.32280137365271\n",
      "Iteration 77, the loss is 32.70383297527346, parameters k is 19.936747779688414 and b is -70.3326037451942\n",
      "Iteration 78, the loss is 32.310313718009596, parameters k is 19.87478708798881 and b is -70.3424061167357\n",
      "Iteration 79, the loss is 31.91889382025204, parameters k is 19.81298995360146 and b is -70.35216896258551\n",
      "Iteration 80, the loss is 31.527473922494522, parameters k is 19.751192819214108 and b is -70.36193180843532\n",
      "Iteration 81, the loss is 31.136054024736953, parameters k is 19.689395684826756 and b is -70.37169465428512\n",
      "Iteration 82, the loss is 30.74463412697939, parameters k is 19.627598550439405 and b is -70.38145750013493\n",
      "Iteration 83, the loss is 30.35321422922188, parameters k is 19.565801416052054 and b is -70.39122034598473\n",
      "Iteration 84, the loss is 29.96179433146432, parameters k is 19.504004281664702 and b is -70.40098319183454\n",
      "Iteration 85, the loss is 29.570374433706817, parameters k is 19.44220714727735 and b is -70.41074603768435\n",
      "Iteration 86, the loss is 29.17895453594923, parameters k is 19.38041001289 and b is -70.42050888353415\n",
      "Iteration 87, the loss is 28.788901334146697, parameters k is 19.31861287850265 and b is -70.43027172938396\n",
      "Iteration 88, the loss is 28.400589028962717, parameters k is 19.2570614358149 and b is -70.43999504954206\n",
      "Iteration 89, the loss is 28.012276723778665, parameters k is 19.195509993127153 and b is -70.44971836970016\n",
      "Iteration 90, the loss is 27.623964418594703, parameters k is 19.133958550439406 and b is -70.45944168985827\n",
      "Iteration 91, the loss is 27.23605553204843, parameters k is 19.07240710775166 and b is -70.46916501001637\n",
      "Iteration 92, the loss is 26.8500684469897, parameters k is 19.011038589965096 and b is -70.47884880448277\n",
      "Iteration 93, the loss is 26.46408136193099, parameters k is 18.949670072178534 and b is -70.48853259894916\n",
      "Iteration 94, the loss is 26.078094276872307, parameters k is 18.888301554391973 and b is -70.49821639341556\n",
      "Iteration 95, the loss is 25.69210719181357, parameters k is 18.82693303660541 and b is -70.50790018788196\n",
      "Iteration 96, the loss is 25.306120106754904, parameters k is 18.76556451881885 and b is -70.51758398234836\n",
      "Iteration 97, the loss is 24.920133021696206, parameters k is 18.704196001032287 and b is -70.52726777681475\n",
      "Iteration 98, the loss is 24.534145936637483, parameters k is 18.642827483245725 and b is -70.53695157128115\n",
      "Iteration 99, the loss is 24.14905550976131, parameters k is 18.581458965459163 and b is -70.54663536574755\n",
      "Iteration 100, the loss is 23.765548655528256, parameters k is 18.520286613680508 and b is -70.55627963452226\n",
      "Iteration 101, the loss is 23.382041801295212, parameters k is 18.459114261901853 and b is -70.56592390329696\n",
      "Iteration 102, the loss is 22.998534947062208, parameters k is 18.397941910123198 and b is -70.57556817207167\n",
      "Iteration 103, the loss is 22.61502809282918, parameters k is 18.336769558344542 and b is -70.58521244084638\n",
      "Iteration 104, the loss is 22.231521238596116, parameters k is 18.275597206565887 and b is -70.59485670962108\n",
      "Iteration 105, the loss is 21.848014384363108, parameters k is 18.214424854787232 and b is -70.60450097839579\n",
      "Iteration 106, the loss is 21.464984756667707, parameters k is 18.153252503008577 and b is -70.6141452471705\n",
      "Iteration 107, the loss is 21.083663264290685, parameters k is 18.092252799451266 and b is -70.6237499902535\n",
      "Iteration 108, the loss is 20.70285754382464, parameters k is 18.031253095893955 and b is -70.6333547333365\n",
      "Iteration 109, the loss is 20.32482746125962, parameters k is 17.97051754253427 and b is -70.6429199507278\n",
      "Iteration 110, the loss is 19.946797378694654, parameters k is 17.909781989174586 and b is -70.6524851681191\n",
      "Iteration 111, the loss is 19.56876729612966, parameters k is 17.849046435814902 and b is -70.6620503855104\n",
      "Iteration 112, the loss is 19.19073721356467, parameters k is 17.788310882455217 and b is -70.6716156029017\n",
      "Iteration 113, the loss is 18.81270713099971, parameters k is 17.727575329095533 and b is -70.681180820293\n",
      "Iteration 114, the loss is 18.434677048434693, parameters k is 17.66683977573585 and b is -70.69074603768429\n",
      "Iteration 115, the loss is 18.05664696586969, parameters k is 17.606104222376164 and b is -70.70031125507559\n",
      "Iteration 116, the loss is 17.679468460287417, parameters k is 17.54536866901648 and b is -70.70987647246689\n",
      "Iteration 117, the loss is 17.304201452053842, parameters k is 17.484854775735847 and b is -70.7194021641665\n",
      "Iteration 118, the loss is 16.930819699623118, parameters k is 17.424340882455216 and b is -70.7289278558661\n",
      "Iteration 119, the loss is 16.56392852661955, parameters k is 17.36442133700067 and b is -70.73833497049061\n",
      "Iteration 120, the loss is 16.20011517118244, parameters k is 17.304744637395927 and b is -70.74770255942342\n",
      "Iteration 121, the loss is 15.840306862446123, parameters k is 17.24548414332478 and b is -70.75699109697283\n",
      "Iteration 122, the loss is 15.482633478161134, parameters k is 17.186223649253634 and b is -70.76627963452223\n",
      "Iteration 123, the loss is 15.131620221848209, parameters k is 17.12761299707972 and b is -70.77544959499654\n",
      "Iteration 124, the loss is 14.782568234963959, parameters k is 17.069200447672603 and b is -70.78458002977915\n",
      "Iteration 125, the loss is 14.438026142944054, parameters k is 17.011204815261536 and b is -70.79363141317836\n",
      "Iteration 126, the loss is 14.094745943537871, parameters k is 16.95320918285047 and b is -70.80268279657757\n",
      "Iteration 127, the loss is 13.755138784078678, parameters k is 16.895623155182484 and b is -70.81165512859339\n",
      "Iteration 128, the loss is 13.418521551850159, parameters k is 16.83824088245521 and b is -70.8205879349175\n",
      "Iteration 129, the loss is 13.087741236956699, parameters k is 16.78127264134849 and b is -70.82944168985821\n",
      "Iteration 130, the loss is 12.760644363724005, parameters k is 16.72473720656588 and b is -70.83821639341552\n",
      "Iteration 131, the loss is 12.436490440267747, parameters k is 16.668415368621215 and b is -70.84695157128114\n",
      "Iteration 132, the loss is 12.116599437938978, parameters k is 16.61252277968841 and b is -70.85560769776335\n",
      "Iteration 133, the loss is 11.798115863025306, parameters k is 16.5566301907556 and b is -70.86426382424555\n",
      "Iteration 134, the loss is 11.483005100701844, parameters k is 16.501154597870226 and b is -70.87284089934437\n",
      "Iteration 135, the loss is 11.167894338378375, parameters k is 16.44567900498485 and b is -70.88141797444318\n",
      "Iteration 136, the loss is 10.852783576054915, parameters k is 16.390203412099474 and b is -70.889995049542\n",
      "Iteration 137, the loss is 10.537672813731456, parameters k is 16.3347278192141 and b is -70.89857212464081\n",
      "Iteration 138, the loss is 10.22390952275637, parameters k is 16.279252226328722 and b is -70.90714919973962\n",
      "Iteration 139, the loss is 9.912379507953915, parameters k is 16.224019835024375 and b is -70.91568674914673\n",
      "Iteration 140, the loss is 9.60510085735786, parameters k is 16.169236257949276 and b is -70.92414524717044\n",
      "Iteration 141, the loss is 9.301504244146587, parameters k is 16.114452680874177 and b is -70.93260374519416\n",
      "Iteration 142, the loss is 9.010246465855182, parameters k is 16.060921831071806 and b is -70.94086461475938\n",
      "Iteration 143, the loss is 8.725117067080891, parameters k is 16.00808248324572 and b is -70.9490069072495\n",
      "Iteration 144, the loss is 8.446604389208325, parameters k is 15.955677997079711 and b is -70.95707014835622\n",
      "Iteration 145, the loss is 8.177492477811194, parameters k is 15.904190744115285 and b is -70.96497528669614\n",
      "Iteration 146, the loss is 7.916300809847678, parameters k is 15.853374084036233 and b is -70.97276184796097\n",
      "Iteration 147, the loss is 7.6661236138226165, parameters k is 15.803478925933465 and b is -70.98039030645899\n",
      "Iteration 148, the loss is 7.430470990656695, parameters k is 15.75515894569631 and b is -70.98774208511512\n",
      "Iteration 149, the loss is 7.20886301669829, parameters k is 15.707991534629116 and b is -70.99489623531275\n",
      "Iteration 150, the loss is 7.006232028360577, parameters k is 15.663156297474966 and b is -71.00165512859338\n",
      "Iteration 151, the loss is 6.813825735157057, parameters k is 15.619441890360342 and b is -71.00821639341552\n",
      "Iteration 152, the loss is 6.6405910540507564, parameters k is 15.577787641348484 and b is -71.01442192701236\n",
      "Iteration 153, the loss is 6.483407738553911, parameters k is 15.538011692731883 and b is -71.02031125507561\n",
      "Iteration 154, the loss is 6.345511024156265, parameters k is 15.500901870597495 and b is -71.02576580053015\n",
      "Iteration 155, the loss is 6.219457230191219, parameters k is 15.465540190755599 and b is -71.0309436661428\n",
      "Iteration 156, the loss is 6.1025181547421665, parameters k is 15.431125665063899 and b is -71.03596342898865\n",
      "Iteration 157, the loss is 6.003716261327245, parameters k is 15.399698570202238 and b is -71.0405088835341\n",
      "Iteration 158, the loss is 5.910565700885357, parameters k is 15.369224597870222 and b is -71.04489623531276\n",
      "Iteration 159, the loss is 5.825245253741783, parameters k is 15.340010546486823 and b is -71.04908595863292\n",
      "Iteration 160, the loss is 5.74791769001632, parameters k is 15.311973491150855 and b is -71.05307805349457\n",
      "Iteration 161, the loss is 5.681197045760212, parameters k is 15.285925882455203 and b is -71.05675394282264\n",
      "Iteration 162, the loss is 5.62272839807461, parameters k is 15.261385546486823 and b is -71.0601926780005\n",
      "Iteration 163, the loss is 5.570890546843075, parameters k is 15.238712483245717 and b is -71.06335473333647\n",
      "Iteration 164, the loss is 5.522726579211203, parameters k is 15.216309222376152 and b is -71.06647726298074\n",
      "Iteration 165, the loss is 5.48367737042422, parameters k is 15.196410309332673 and b is -71.06920453570801\n",
      "Iteration 166, the loss is 5.450922375261663, parameters k is 15.178192305380104 and b is -71.07165512859339\n",
      "Iteration 167, the loss is 5.421849346232791, parameters k is 15.161025645301054 and b is -71.07394761871197\n",
      "Iteration 168, the loss is 5.396217244472228, parameters k is 15.145106297474968 and b is -71.07604248037205\n",
      "Iteration 169, the loss is 5.372717734793183, parameters k is 15.129659360716074 and b is -71.07805829064873\n",
      "Iteration 170, the loss is 5.3518525020563015, parameters k is 15.115155309332675 and b is -71.07991599815861\n",
      "Iteration 171, the loss is 5.331985948918926, parameters k is 15.101140072178525 and b is -71.08169465428509\n",
      "Iteration 172, the loss is 5.312974856484837, parameters k is 15.087343016842556 and b is -71.08343378471987\n",
      "Iteration 173, the loss is 5.296299425112516, parameters k is 15.07430509194137 and b is -71.08505433807956\n",
      "Iteration 174, the loss is 5.281892467496414, parameters k is 15.06195704846311 and b is -71.08655631436415\n",
      "Iteration 175, the loss is 5.270101508499169, parameters k is 15.050950665063901 and b is -71.08786066219024\n",
      "Iteration 176, the loss is 5.259935686442725, parameters k is 15.040884242139 and b is -71.08900690724953\n",
      "Iteration 177, the loss is 5.251010544723787, parameters k is 15.031053708542162 and b is -71.09011362661711\n",
      "Iteration 178, the loss is 5.243705157878908, parameters k is 15.022270131467064 and b is -71.0910622432179\n",
      "Iteration 179, the loss is 5.2381982906185565, parameters k is 15.014726949648882 and b is -71.09181323136019\n",
      "Iteration 180, the loss is 5.233746891617743, parameters k is 15.007948273759554 and b is -71.09244564242738\n",
      "Iteration 181, the loss is 5.230187811180183, parameters k is 15.001708807356392 and b is -71.09299900211117\n",
      "Iteration 182, the loss is 5.227143293624316, parameters k is 14.996203609727933 and b is -71.09343378471986\n",
      "Iteration 183, the loss is 5.224425200805009, parameters k is 14.990936396289198 and b is -71.09382904163687\n",
      "Iteration 184, the loss is 5.221977985358297, parameters k is 14.985928036605404 and b is -71.09418477286216\n",
      "Iteration 185, the loss is 5.219730512443761, parameters k is 14.981156989174574 and b is -71.09450097839576\n",
      "Iteration 186, the loss is 5.217944665313121, parameters k is 14.976627838976945 and b is -71.09477765823766\n",
      "Iteration 187, the loss is 5.2169124395182065, parameters k is 14.973316672969041 and b is -71.09485670962106\n",
      "Iteration 188, the loss is 5.215996253002825, parameters k is 14.970289222376156 and b is -71.09489623531276\n",
      "Iteration 189, the loss is 5.215215426070331, parameters k is 14.967494894312916 and b is -71.09489623531276\n",
      "Iteration 190, the loss is 5.214480586325735, parameters k is 14.964700566249675 and b is -71.09489623531276\n",
      "Iteration 191, the loss is 5.21383155277747, parameters k is 14.96214892593347 and b is -71.09485670962106\n",
      "Iteration 192, the loss is 5.213296979985794, parameters k is 14.959838194708173 and b is -71.09477765823766\n",
      "Iteration 193, the loss is 5.212762407194124, parameters k is 14.957527463482876 and b is -71.09469860685427\n",
      "Iteration 194, the loss is 5.212227834402443, parameters k is 14.955216732257579 and b is -71.09461955547087\n",
      "Iteration 195, the loss is 5.211735398591575, parameters k is 14.952906001032282 and b is -71.09454050408748\n",
      "Iteration 196, the loss is 5.211300027890542, parameters k is 14.9508228192141 and b is -71.09442192701238\n",
      "Iteration 197, the loss is 5.210864657189508, parameters k is 14.948739637395917 and b is -71.09430334993728\n",
      "Iteration 198, the loss is 5.210429286488466, parameters k is 14.946656455577735 and b is -71.09418477286218\n",
      "Iteration 199, the loss is 5.209993915787438, parameters k is 14.944573273759552 and b is -71.09406619578708\n",
      "Iteration 200, the loss is 5.209594627342856, parameters k is 14.94249009194137 and b is -71.09394761871198\n",
      "Iteration 201, the loss is 5.209253909426274, parameters k is 14.940651020795125 and b is -71.09378951594518\n",
      "Iteration 202, the loss is 5.208913191509694, parameters k is 14.93881194964888 and b is -71.09363141317837\n",
      "Iteration 203, the loss is 5.20857247359311, parameters k is 14.936972878502635 and b is -71.09347331041157\n",
      "Iteration 204, the loss is 5.20823175567653, parameters k is 14.93513380735639 and b is -71.09331520764476\n",
      "Iteration 205, the loss is 5.207891037759946, parameters k is 14.933294736210145 and b is -71.09315710487796\n",
      "Iteration 206, the loss is 5.207592384666513, parameters k is 14.9314556650639 and b is -71.09299900211116\n",
      "Iteration 207, the loss is 5.207460547862382, parameters k is 14.930135368621213 and b is -71.09276184796096\n",
      "Iteration 208, the loss is 5.207390731577763, parameters k is 14.929361949648882 and b is -71.09244564242736\n",
      "Iteration 209, the loss is 5.207320915293135, parameters k is 14.928588530676551 and b is -71.09212943689377\n",
      "Iteration 210, the loss is 5.207266371401401, parameters k is 14.92781511170422 and b is -71.09181323136018\n",
      "Iteration 211, the loss is 5.207227761329332, parameters k is 14.927305645301058 and b is -71.09145750013488\n",
      "Iteration 212, the loss is 5.20718915125727, parameters k is 14.926796178897895 and b is -71.09110176890958\n",
      "Iteration 213, the loss is 5.207150541185218, parameters k is 14.926286712494733 and b is -71.09074603768428\n",
      "Iteration 214, the loss is 5.207111931113154, parameters k is 14.92577724609157 and b is -71.09039030645899\n",
      "Iteration 215, the loss is 5.207073321041097, parameters k is 14.925267779688408 and b is -71.09003457523369\n",
      "Iteration 216, the loss is 5.207034710969037, parameters k is 14.924758313285245 and b is -71.08967884400839\n",
      "Iteration 217, the loss is 5.206996100896965, parameters k is 14.924248846882083 and b is -71.08932311278309\n",
      "Iteration 218, the loss is 5.206957490824916, parameters k is 14.92373938047892 and b is -71.0889673815578\n",
      "Iteration 219, the loss is 5.206918880752856, parameters k is 14.923229914075758 and b is -71.0886116503325\n",
      "Iteration 220, the loss is 5.20688027068079, parameters k is 14.922720447672596 and b is -71.0882559191072\n",
      "Iteration 221, the loss is 5.206845704548096, parameters k is 14.922210981269433 and b is -71.0879001878819\n",
      "Iteration 222, the loss is 5.206824629612346, parameters k is 14.921977483245717 and b is -71.0875049309649\n",
      "Iteration 223, the loss is 5.206803554676592, parameters k is 14.921743985222001 and b is -71.0871096740479\n",
      "Iteration 224, the loss is 5.206782479740842, parameters k is 14.921510487198285 and b is -71.08671441713089\n",
      "Iteration 225, the loss is 5.206761404805089, parameters k is 14.921276989174569 and b is -71.08631916021389\n",
      "Iteration 226, the loss is 5.206740329869336, parameters k is 14.921043491150852 and b is -71.08592390329689\n",
      "Iteration 227, the loss is 5.206719254933584, parameters k is 14.920809993127136 and b is -71.08552864637988\n",
      "Iteration 228, the loss is 5.2066981799978365, parameters k is 14.92057649510342 and b is -71.08513338946288\n",
      "Iteration 229, the loss is 5.206677105062088, parameters k is 14.920342997079704 and b is -71.08473813254588\n",
      "Iteration 230, the loss is 5.206656030126335, parameters k is 14.920109499055988 and b is -71.08434287562888\n",
      "Iteration 231, the loss is 5.206634955190577, parameters k is 14.919876001032272 and b is -71.08394761871187\n",
      "Iteration 232, the loss is 5.2066138802548325, parameters k is 14.919642503008555 and b is -71.08355236179487\n",
      "Iteration 233, the loss is 5.206592805319074, parameters k is 14.91940900498484 and b is -71.08315710487787\n",
      "Iteration 234, the loss is 5.206571730383323, parameters k is 14.919175506961123 and b is -71.08276184796087\n",
      "Iteration 235, the loss is 5.20655065544758, parameters k is 14.918942008937407 and b is -71.08236659104386\n",
      "Iteration 236, the loss is 5.206529580511822, parameters k is 14.91870851091369 and b is -71.08197133412686\n",
      "Iteration 237, the loss is 5.206508505576073, parameters k is 14.918475012889974 and b is -71.08157607720986\n",
      "Iteration 238, the loss is 5.206487430640321, parameters k is 14.918241514866258 and b is -71.08118082029286\n",
      "Iteration 239, the loss is 5.206466355704564, parameters k is 14.918008016842542 and b is -71.08078556337585\n",
      "Iteration 240, the loss is 5.206445280768819, parameters k is 14.917774518818826 and b is -71.08039030645885\n",
      "Iteration 241, the loss is 5.206424205833065, parameters k is 14.91754102079511 and b is -71.07999504954185\n",
      "Iteration 242, the loss is 5.20640313089731, parameters k is 14.917307522771393 and b is -71.07959979262485\n",
      "Iteration 243, the loss is 5.206382055961572, parameters k is 14.917074024747677 and b is -71.07920453570785\n",
      "Iteration 244, the loss is 5.206360981025814, parameters k is 14.916840526723961 and b is -71.07880927879084\n",
      "Iteration 245, the loss is 5.206339906090059, parameters k is 14.916607028700245 and b is -71.07841402187384\n",
      "Iteration 246, the loss is 5.2063188311543085, parameters k is 14.916373530676529 and b is -71.07801876495684\n",
      "Iteration 247, the loss is 5.206297756218558, parameters k is 14.916140032652812 and b is -71.07762350803984\n",
      "Iteration 248, the loss is 5.2062766812828105, parameters k is 14.915906534629096 and b is -71.07722825112283\n",
      "Iteration 249, the loss is 5.20625560634705, parameters k is 14.91567303660538 and b is -71.07683299420583\n",
      "Iteration 250, the loss is 5.206234531411304, parameters k is 14.915439538581664 and b is -71.07643773728883\n",
      "Iteration 251, the loss is 5.2062134564755445, parameters k is 14.915206040557948 and b is -71.07604248037183\n",
      "Iteration 252, the loss is 5.2061923815397995, parameters k is 14.914972542534231 and b is -71.07564722345482\n",
      "Iteration 253, the loss is 5.206171306604051, parameters k is 14.914739044510515 and b is -71.07525196653782\n",
      "Iteration 254, the loss is 5.206150231668295, parameters k is 14.914505546486799 and b is -71.07485670962082\n",
      "Iteration 255, the loss is 5.206129156732551, parameters k is 14.914272048463083 and b is -71.07446145270382\n",
      "Iteration 256, the loss is 5.206108081796788, parameters k is 14.914038550439367 and b is -71.07406619578681\n",
      "Iteration 257, the loss is 5.206087006861042, parameters k is 14.91380505241565 and b is -71.07367093886981\n",
      "Iteration 258, the loss is 5.206065931925293, parameters k is 14.913571554391934 and b is -71.07327568195281\n",
      "Iteration 259, the loss is 5.2060448569895375, parameters k is 14.913338056368218 and b is -71.0728804250358\n",
      "Iteration 260, the loss is 5.206023782053787, parameters k is 14.913104558344502 and b is -71.0724851681188\n",
      "Iteration 261, the loss is 5.206002707118039, parameters k is 14.912871060320786 and b is -71.0720899112018\n",
      "Iteration 262, the loss is 5.205981632182284, parameters k is 14.91263756229707 and b is -71.0716946542848\n",
      "Iteration 263, the loss is 5.205960557246531, parameters k is 14.912404064273353 and b is -71.0712993973678\n",
      "Iteration 264, the loss is 5.205939482310785, parameters k is 14.912170566249637 and b is -71.07090414045079\n",
      "Iteration 265, the loss is 5.205918407375035, parameters k is 14.911937068225921 and b is -71.07050888353379\n",
      "Iteration 266, the loss is 5.205897332439281, parameters k is 14.911703570202205 and b is -71.07011362661679\n",
      "Iteration 267, the loss is 5.205876257503527, parameters k is 14.911470072178489 and b is -71.06971836969979\n",
      "Iteration 268, the loss is 5.205855182567777, parameters k is 14.911236574154772 and b is -71.06932311278278\n",
      "Iteration 269, the loss is 5.205834107632028, parameters k is 14.911003076131056 and b is -71.06892785586578\n",
      "Iteration 270, the loss is 5.20581303269627, parameters k is 14.91076957810734 and b is -71.06853259894878\n",
      "Iteration 271, the loss is 5.205791957760523, parameters k is 14.910536080083624 and b is -71.06813734203178\n",
      "Iteration 272, the loss is 5.20577088282477, parameters k is 14.910302582059908 and b is -71.06774208511477\n",
      "Iteration 273, the loss is 5.205749807889017, parameters k is 14.910069084036191 and b is -71.06734682819777\n",
      "Iteration 274, the loss is 5.205728732953262, parameters k is 14.909835586012475 and b is -71.06695157128077\n",
      "Iteration 275, the loss is 5.205707658017517, parameters k is 14.909602087988759 and b is -71.06655631436377\n",
      "Iteration 276, the loss is 5.205686583081758, parameters k is 14.909368589965043 and b is -71.06616105744676\n",
      "Iteration 277, the loss is 5.205665508146011, parameters k is 14.909135091941327 and b is -71.06576580052976\n",
      "Iteration 278, the loss is 5.20564443321026, parameters k is 14.90890159391761 and b is -71.06537054361276\n",
      "Iteration 279, the loss is 5.205623358274507, parameters k is 14.908668095893894 and b is -71.06497528669576\n",
      "Iteration 280, the loss is 5.205602283338757, parameters k is 14.908434597870178 and b is -71.06458002977875\n",
      "Iteration 281, the loss is 5.205581208403009, parameters k is 14.908201099846462 and b is -71.06418477286175\n",
      "Iteration 282, the loss is 5.205560133467254, parameters k is 14.907967601822746 and b is -71.06378951594475\n",
      "Iteration 283, the loss is 5.205539058531507, parameters k is 14.90773410379903 and b is -71.06339425902775\n",
      "Iteration 284, the loss is 5.205517983595755, parameters k is 14.907500605775313 and b is -71.06299900211074\n",
      "Iteration 285, the loss is 5.20549690866, parameters k is 14.907267107751597 and b is -71.06260374519374\n",
      "Iteration 286, the loss is 5.205475833724246, parameters k is 14.90703360972788 and b is -71.06220848827674\n",
      "Iteration 287, the loss is 5.205454758788494, parameters k is 14.906800111704165 and b is -71.06181323135974\n",
      "Iteration 288, the loss is 5.205433683852743, parameters k is 14.906566613680448 and b is -71.06141797444273\n",
      "Iteration 289, the loss is 5.205412608916999, parameters k is 14.906333115656732 and b is -71.06102271752573\n",
      "Iteration 290, the loss is 5.205391533981243, parameters k is 14.906099617633016 and b is -71.06062746060873\n",
      "Iteration 291, the loss is 5.205370459045485, parameters k is 14.9058661196093 and b is -71.06023220369173\n",
      "Iteration 292, the loss is 5.205349384109737, parameters k is 14.905632621585584 and b is -71.05983694677472\n",
      "Iteration 293, the loss is 5.205328309173989, parameters k is 14.905399123561867 and b is -71.05944168985772\n",
      "Iteration 294, the loss is 5.205307234238237, parameters k is 14.905165625538151 and b is -71.05904643294072\n",
      "Iteration 295, the loss is 5.2052861593024815, parameters k is 14.904932127514435 and b is -71.05865117602372\n",
      "Iteration 296, the loss is 5.2052650843667285, parameters k is 14.904698629490719 and b is -71.05825591910671\n",
      "Iteration 297, the loss is 5.205244009430977, parameters k is 14.904465131467003 and b is -71.05786066218971\n",
      "Iteration 298, the loss is 5.205222934495233, parameters k is 14.904231633443286 and b is -71.05746540527271\n",
      "Iteration 299, the loss is 5.205201859559477, parameters k is 14.90399813541957 and b is -71.0570701483557\n",
      "Iteration 300, the loss is 5.205180784623729, parameters k is 14.903764637395854 and b is -71.0566748914387\n",
      "Iteration 301, the loss is 5.205159709687976, parameters k is 14.903531139372138 and b is -71.0562796345217\n",
      "Iteration 302, the loss is 5.2051386347522195, parameters k is 14.903297641348422 and b is -71.0558843776047\n",
      "Iteration 303, the loss is 5.205117559816474, parameters k is 14.903064143324706 and b is -71.0554891206877\n",
      "Iteration 304, the loss is 5.205096484880723, parameters k is 14.90283064530099 and b is -71.0550938637707\n",
      "Iteration 305, the loss is 5.205075409944963, parameters k is 14.902597147277273 and b is -71.05469860685369\n",
      "Iteration 306, the loss is 5.205054335009217, parameters k is 14.902363649253557 and b is -71.05430334993669\n",
      "Iteration 307, the loss is 5.205033260073468, parameters k is 14.90213015122984 and b is -71.05390809301969\n",
      "Iteration 308, the loss is 5.205014320981633, parameters k is 14.901896653206125 and b is -71.05351283610268\n",
      "Iteration 309, the loss is 5.204995405097893, parameters k is 14.901885566249604 and b is -71.05307805349399\n",
      "Iteration 310, the loss is 5.2049772972145325, parameters k is 14.901874479293083 and b is -71.0526432708853\n",
      "Iteration 311, the loss is 5.204959045252559, parameters k is 14.901640981269367 and b is -71.0522480139683\n",
      "Iteration 312, the loss is 5.204940129368818, parameters k is 14.901629894312846 and b is -71.0518132313596\n",
      "Iteration 313, the loss is 5.2049213343556, parameters k is 14.901618807356325 and b is -71.05137844875091\n",
      "Iteration 314, the loss is 5.204903769523479, parameters k is 14.901385309332609 and b is -71.05098319183391\n",
      "Iteration 315, the loss is 5.204884853639738, parameters k is 14.901374222376088 and b is -71.05054840922521\n",
      "Iteration 316, the loss is 5.204865937755998, parameters k is 14.901363135419567 and b is -71.05011362661652\n",
      "Iteration 317, the loss is 5.204847927535081, parameters k is 14.901352048463046 and b is -71.04967884400783\n",
      "Iteration 318, the loss is 5.204829577910664, parameters k is 14.90111855043933 and b is -71.04928358709083\n",
      "Iteration 319, the loss is 5.204810662026916, parameters k is 14.90110746348281 and b is -71.04884880448213\n",
      "Iteration 320, the loss is 5.204791964676139, parameters k is 14.901096376526288 and b is -71.04841402187344\n",
      "Iteration 321, the loss is 5.204774302181585, parameters k is 14.900862878502572 and b is -71.04801876495644\n",
      "Iteration 322, the loss is 5.2047553862978395, parameters k is 14.900851791546051 and b is -71.04758398234775\n",
      "Iteration 323, the loss is 5.204736470414097, parameters k is 14.90084070458953 and b is -71.04714919973905\n",
      "Iteration 324, the loss is 5.2047185578556165, parameters k is 14.90082961763301 and b is -71.04671441713036\n",
      "Iteration 325, the loss is 5.2047001105687665, parameters k is 14.900596119609293 and b is -71.04631916021336\n",
      "Iteration 326, the loss is 5.204681194685021, parameters k is 14.900585032652772 and b is -71.04588437760466\n",
      "Iteration 327, the loss is 5.204662594996681, parameters k is 14.900573945696252 and b is -71.04544959499597\n",
      "Iteration 328, the loss is 5.204644834839686, parameters k is 14.900340447672535 and b is -71.04505433807897\n",
      "Iteration 329, the loss is 5.204625918955943, parameters k is 14.900329360716015 and b is -71.04461955547028\n",
      "Iteration 330, the loss is 5.204607003072206, parameters k is 14.900318273759494 and b is -71.04418477286158\n",
      "Iteration 331, the loss is 5.2045891881761515, parameters k is 14.900307186802973 and b is -71.04374999025289\n",
      "Iteration 332, the loss is 5.20457064322687, parameters k is 14.900073688779257 and b is -71.04335473333589\n",
      "Iteration 333, the loss is 5.204551727343123, parameters k is 14.900062601822736 and b is -71.0429199507272\n",
      "Iteration 334, the loss is 5.204533225317226, parameters k is 14.900051514866215 and b is -71.0424851681185\n",
      "Iteration 335, the loss is 5.204515367497797, parameters k is 14.899818016842499 and b is -71.0420899112015\n",
      "Iteration 336, the loss is 5.204496451614051, parameters k is 14.899806929885978 and b is -71.04165512859281\n",
      "Iteration 337, the loss is 5.2044775357303035, parameters k is 14.899795842929457 and b is -71.04122034598412\n",
      "Iteration 338, the loss is 5.204459818496701, parameters k is 14.899784755972936 and b is -71.04078556337542\n",
      "Iteration 339, the loss is 5.20444117588497, parameters k is 14.89955125794922 and b is -71.04039030645842\n",
      "Iteration 340, the loss is 5.204422260001222, parameters k is 14.8995401709927 and b is -71.03995552384973\n",
      "Iteration 341, the loss is 5.20440385563776, parameters k is 14.899529084036178 and b is -71.03952074124103\n",
      "Iteration 342, the loss is 5.204385900155893, parameters k is 14.899295586012462 and b is -71.03912548432403\n",
      "Iteration 343, the loss is 5.204366984272153, parameters k is 14.899284499055941 and b is -71.03869070171534\n",
      "Iteration 344, the loss is 5.204348068388401, parameters k is 14.89927341209942 and b is -71.03825591910665\n",
      "Iteration 345, the loss is 5.204330448817239, parameters k is 14.8992623251429 and b is -71.03782113649795\n",
      "Iteration 346, the loss is 5.2043117085430755, parameters k is 14.899028827119183 and b is -71.03742587958095\n",
      "Iteration 347, the loss is 5.204292792659329, parameters k is 14.899017740162662 and b is -71.03699109697226\n",
      "Iteration 348, the loss is 5.2042744859583046, parameters k is 14.899006653206142 and b is -71.03655631436357\n",
      "Iteration 349, the loss is 5.204256432813998, parameters k is 14.898773155182425 and b is -71.03616105744656\n",
      "Iteration 350, the loss is 5.204237516930248, parameters k is 14.898762068225905 and b is -71.03572627483787\n",
      "Iteration 351, the loss is 5.2042186010465095, parameters k is 14.898750981269384 and b is -71.03529149222918\n",
      "Iteration 352, the loss is 5.2042010791377855, parameters k is 14.898739894312863 and b is -71.03485670962048\n",
      "Iteration 353, the loss is 5.2041822412011784, parameters k is 14.898506396289147 and b is -71.03446145270348\n",
      "Iteration 354, the loss is 5.204163325317435, parameters k is 14.898495309332626 and b is -71.03402667009479\n",
      "Iteration 355, the loss is 5.204145116278841, parameters k is 14.898484222376105 and b is -71.0335918874861\n",
      "Iteration 356, the loss is 5.204126965472096, parameters k is 14.898250724352389 and b is -71.0331966305691\n",
      "Iteration 357, the loss is 5.204108049588354, parameters k is 14.898239637395868 and b is -71.0327618479604\n",
      "Iteration 358, the loss is 5.204089153419913, parameters k is 14.898228550439347 and b is -71.03232706535171\n",
      "Iteration 359, the loss is 5.204071689743024, parameters k is 14.89799505241563 and b is -71.0319318084347\n",
      "Iteration 360, the loss is 5.204052773859279, parameters k is 14.89798396545911 and b is -71.03149702582601\n",
      "Iteration 361, the loss is 5.204033857975536, parameters k is 14.897972878502589 and b is -71.03106224321732\n",
      "Iteration 362, the loss is 5.204015746599384, parameters k is 14.897961791546068 and b is -71.03062746060863\n",
      "Iteration 363, the loss is 5.203997498130205, parameters k is 14.897728293522352 and b is -71.03023220369163\n",
      "Iteration 364, the loss is 5.203978582246458, parameters k is 14.897717206565831 and b is -71.02979742108293\n",
      "Iteration 365, the loss is 5.2039597837404505, parameters k is 14.89770611960931 and b is -71.02936263847424\n",
      "Iteration 366, the loss is 5.2039422224011265, parameters k is 14.897472621585594 and b is -71.02896738155724\n",
      "Iteration 367, the loss is 5.203923306517386, parameters k is 14.897461534629073 and b is -71.02853259894854\n",
      "Iteration 368, the loss is 5.203904390633637, parameters k is 14.897450447672552 and b is -71.02809781633985\n",
      "Iteration 369, the loss is 5.203886376919929, parameters k is 14.897439360716032 and b is -71.02766303373116\n",
      "Iteration 370, the loss is 5.203868030788305, parameters k is 14.897205862692315 and b is -71.02726777681416\n",
      "Iteration 371, the loss is 5.203849114904559, parameters k is 14.897194775735795 and b is -71.02683299420546\n",
      "Iteration 372, the loss is 5.203830414060996, parameters k is 14.897183688779274 and b is -71.02639821159677\n",
      "Iteration 373, the loss is 5.203812755059225, parameters k is 14.896950190755557 and b is -71.02600295467977\n",
      "Iteration 374, the loss is 5.203793839175485, parameters k is 14.896939103799037 and b is -71.02556817207108\n",
      "Iteration 375, the loss is 5.203774923291743, parameters k is 14.896928016842516 and b is -71.02513338946238\n",
      "Iteration 376, the loss is 5.203757007240471, parameters k is 14.896916929885995 and b is -71.02469860685369\n",
      "Iteration 377, the loss is 5.2037385634464055, parameters k is 14.896683431862279 and b is -71.02430334993669\n",
      "Iteration 378, the loss is 5.203719647562663, parameters k is 14.896672344905758 and b is -71.023868567328\n",
      "Iteration 379, the loss is 5.203701044381531, parameters k is 14.896661257949237 and b is -71.0234337847193\n",
      "Iteration 380, the loss is 5.2036832877173325, parameters k is 14.89642775992552 and b is -71.0230385278023\n",
      "Iteration 381, the loss is 5.203664371833588, parameters k is 14.896416672969 and b is -71.0226037451936\n",
      "Iteration 382, the loss is 5.203645455949845, parameters k is 14.896405586012479 and b is -71.02216896258491\n",
      "Iteration 383, the loss is 5.20362763756101, parameters k is 14.896394499055958 and b is -71.02173417997622\n",
      "Iteration 384, the loss is 5.2036090961045085, parameters k is 14.896161001032242 and b is -71.02133892305922\n",
      "Iteration 385, the loss is 5.203590180220766, parameters k is 14.896149914075721 and b is -71.02090414045053\n",
      "Iteration 386, the loss is 5.2035716747020695, parameters k is 14.8961388271192 and b is -71.02046935784183\n",
      "Iteration 387, the loss is 5.203553820375429, parameters k is 14.895905329095484 and b is -71.02007410092483\n",
      "Iteration 388, the loss is 5.203534904491691, parameters k is 14.895894242138963 and b is -71.01963931831614\n",
      "Iteration 389, the loss is 5.203515988607947, parameters k is 14.895883155182442 and b is -71.01920453570744\n",
      "Iteration 390, the loss is 5.203498267881544, parameters k is 14.895872068225922 and b is -71.01876975309875\n",
      "Iteration 391, the loss is 5.203479628762614, parameters k is 14.895638570202205 and b is -71.01837449618175\n",
      "Iteration 392, the loss is 5.2034607128788695, parameters k is 14.895627483245685 and b is -71.01793971357306\n",
      "Iteration 393, the loss is 5.203442305022621, parameters k is 14.895616396289164 and b is -71.01750493096436\n",
      "Iteration 394, the loss is 5.2034243530335385, parameters k is 14.895382898265447 and b is -71.01710967404736\n",
      "Iteration 395, the loss is 5.20340543714979, parameters k is 14.895371811308927 and b is -71.01667489143867\n",
      "Iteration 396, the loss is 5.2033865212660535, parameters k is 14.895360724352406 and b is -71.01624010882998\n",
      "Iteration 397, the loss is 5.203368898202091, parameters k is 14.895349637395885 and b is -71.01580532622128\n",
      "Iteration 398, the loss is 5.203350161420713, parameters k is 14.895116139372169 and b is -71.01541006930428\n",
      "Iteration 399, the loss is 5.203331245536968, parameters k is 14.895105052415648 and b is -71.01497528669559\n",
      "Iteration 400, the loss is 5.203312935343158, parameters k is 14.895093965459127 and b is -71.0145405040869\n",
      "Iteration 401, the loss is 5.203294885691637, parameters k is 14.89486046743541 and b is -71.01414524716989\n",
      "Iteration 402, the loss is 5.203275969807898, parameters k is 14.89484938047889 and b is -71.0137104645612\n",
      "Iteration 403, the loss is 5.203257053924153, parameters k is 14.894838293522369 and b is -71.0132756819525\n",
      "Iteration 404, the loss is 5.203239528522634, parameters k is 14.894827206565848 and b is -71.01284089934381\n",
      "Iteration 405, the loss is 5.203220694078822, parameters k is 14.894593708542132 and b is -71.01244564242681\n",
      "Iteration 406, the loss is 5.203201778195082, parameters k is 14.894582621585611 and b is -71.01201085981812\n",
      "Iteration 407, the loss is 5.2031835656637, parameters k is 14.89457153462909 and b is -71.01157607720943\n",
      "Iteration 408, the loss is 5.203165418349741, parameters k is 14.894338036605374 and b is -71.01118082029242\n",
      "Iteration 409, the loss is 5.203146502465999, parameters k is 14.894326949648853 and b is -71.01074603768373\n",
      "Iteration 410, the loss is 5.203127602804766, parameters k is 14.894315862692332 and b is -71.01031125507504\n",
      "Iteration 411, the loss is 5.203110142620661, parameters k is 14.894082364668616 and b is -71.00991599815804\n",
      "Iteration 412, the loss is 5.203091226736921, parameters k is 14.894071277712095 and b is -71.00948121554934\n",
      "Iteration 413, the loss is 5.2030723108531785, parameters k is 14.894060190755575 and b is -71.00904643294065\n",
      "Iteration 414, the loss is 5.2030541959842385, parameters k is 14.894049103799054 and b is -71.00861165033196\n",
      "Iteration 415, the loss is 5.203035951007842, parameters k is 14.893815605775337 and b is -71.00821639341495\n",
      "Iteration 416, the loss is 5.203017035124106, parameters k is 14.893804518818817 and b is -71.00778161080626\n",
      "Iteration 417, the loss is 5.20299823312531, parameters k is 14.893793431862296 and b is -71.00734682819757\n",
      "Iteration 418, the loss is 5.202980675278765, parameters k is 14.89355993383858 and b is -71.00695157128057\n",
      "Iteration 419, the loss is 5.202961759395025, parameters k is 14.893548846882059 and b is -71.00651678867187\n",
      "Iteration 420, the loss is 5.202942843511282, parameters k is 14.893537759925538 and b is -71.00608200606318\n",
      "Iteration 421, the loss is 5.202924826304785, parameters k is 14.893526672969017 and b is -71.00564722345449\n",
      "Iteration 422, the loss is 5.202906483665945, parameters k is 14.8932931749453 and b is -71.00525196653749\n",
      "Iteration 423, the loss is 5.2028875677822075, parameters k is 14.89328208798878 and b is -71.0048171839288\n",
      "Iteration 424, the loss is 5.202868863445848, parameters k is 14.893271001032259 and b is -71.0043824013201\n",
      "Iteration 425, the loss is 5.2028512079368685, parameters k is 14.893037503008543 and b is -71.0039871444031\n",
      "Iteration 426, the loss is 5.202832292053126, parameters k is 14.893026416052022 and b is -71.0035523617944\n",
      "Iteration 427, the loss is 5.202813376169384, parameters k is 14.893015329095501 and b is -71.00311757918571\n",
      "Iteration 428, the loss is 5.202795456625324, parameters k is 14.89300424213898 and b is -71.00268279657702\n",
      "Iteration 429, the loss is 5.202777016324047, parameters k is 14.892770744115264 and b is -71.00228753966002\n",
      "Iteration 430, the loss is 5.202758100440309, parameters k is 14.892759657158743 and b is -71.00185275705132\n",
      "Iteration 431, the loss is 5.2027394937663844, parameters k is 14.892748570202222 and b is -71.00141797444263\n",
      "Iteration 432, the loss is 5.202721740594976, parameters k is 14.892515072178506 and b is -71.00102271752563\n",
      "Iteration 433, the loss is 5.202702824711229, parameters k is 14.892503985221985 and b is -71.00058793491694\n",
      "Iteration 434, the loss is 5.202683908827484, parameters k is 14.892492898265465 and b is -71.00015315230824\n",
      "Iteration 435, the loss is 5.202666086945867, parameters k is 14.892481811308944 and b is -70.99971836969955\n",
      "Iteration 436, the loss is 5.202647548982148, parameters k is 14.892248313285227 and b is -70.99932311278255\n",
      "Iteration 437, the loss is 5.202628633098406, parameters k is 14.892237226328707 and b is -70.99888833017386\n",
      "Iteration 438, the loss is 5.202610124086926, parameters k is 14.892226139372186 and b is -70.99845354756516\n",
      "Iteration 439, the loss is 5.2025922732530745, parameters k is 14.89199264134847 and b is -70.99805829064816\n",
      "Iteration 440, the loss is 5.202573357369332, parameters k is 14.891981554391949 and b is -70.99762350803947\n",
      "Iteration 441, the loss is 5.202554441485591, parameters k is 14.891970467435428 and b is -70.99718872543077\n",
      "Iteration 442, the loss is 5.20253671726641, parameters k is 14.891959380478907 and b is -70.99675394282208\n",
      "Iteration 443, the loss is 5.202518081640255, parameters k is 14.89172588245519 and b is -70.99635868590508\n",
      "Iteration 444, the loss is 5.202499165756514, parameters k is 14.89171479549867 and b is -70.99592390329639\n",
      "Iteration 445, the loss is 5.202480754407467, parameters k is 14.891703708542149 and b is -70.9954891206877\n",
      "Iteration 446, the loss is 5.20246333514034, parameters k is 14.891470210518433 and b is -70.99509386377069\n",
      "Iteration 447, the loss is 5.20244649868249, parameters k is 14.89170860972792 and b is -70.9946195554703\n",
      "Iteration 448, the loss is 5.202425423746739, parameters k is 14.891475111704203 and b is -70.99422429855329\n",
      "Iteration 449, the loss is 5.2024087933072005, parameters k is 14.891241613680487 and b is -70.99382904163629\n",
      "Iteration 450, the loss is 5.20239116802176, parameters k is 14.891480012889973 and b is -70.99335473333589\n",
      "Iteration 451, the loss is 5.202370093086011, parameters k is 14.891246514866257 and b is -70.99295947641889\n",
      "Iteration 452, the loss is 5.202354251474052, parameters k is 14.89101301684254 and b is -70.99256421950189\n",
      "Iteration 453, the loss is 5.202335837361036, parameters k is 14.891251416052027 and b is -70.99208991120149\n",
      "Iteration 454, the loss is 5.202314762425284, parameters k is 14.89101791802831 and b is -70.99169465428449\n",
      "Iteration 455, the loss is 5.202299709640917, parameters k is 14.890784420004595 and b is -70.99129939736748\n",
      "Iteration 456, the loss is 5.202280506700306, parameters k is 14.891022819214081 and b is -70.99082508906709\n",
      "Iteration 457, the loss is 5.202259431764558, parameters k is 14.890789321190365 and b is -70.99042983215008\n",
      "Iteration 458, the loss is 5.202245167807777, parameters k is 14.890555823166649 and b is -70.99003457523308\n",
      "Iteration 459, the loss is 5.20222517603958, parameters k is 14.890794222376135 and b is -70.98956026693268\n",
      "Iteration 460, the loss is 5.202204101103829, parameters k is 14.890560724352419 and b is -70.98916501001568\n",
      "Iteration 461, the loss is 5.202190625974637, parameters k is 14.890327226328703 and b is -70.98876975309868\n",
      "Iteration 462, the loss is 5.202169845378856, parameters k is 14.890565625538189 and b is -70.98829544479828\n",
      "Iteration 463, the loss is 5.20214926493072, parameters k is 14.890332127514473 and b is -70.98790018788128\n",
      "Iteration 464, the loss is 5.202135589653877, parameters k is 14.890570526723959 and b is -70.98742587958088\n",
      "Iteration 465, the loss is 5.202114514718125, parameters k is 14.890337028700243 and b is -70.98703062266388\n",
      "Iteration 466, the loss is 5.20209472309758, parameters k is 14.890103530676527 and b is -70.98663536574688\n",
      "Iteration 467, the loss is 5.202080258993156, parameters k is 14.890341929886013 and b is -70.98616105744648\n",
      "Iteration 468, the loss is 5.202059184057389, parameters k is 14.890108431862297 and b is -70.98576580052948\n",
      "Iteration 469, the loss is 5.202040181264437, parameters k is 14.88987493383858 and b is -70.98537054361248\n",
      "Iteration 470, the loss is 5.202024928332414, parameters k is 14.890113333048067 and b is -70.98489623531208\n",
      "Iteration 471, the loss is 5.202003853396667, parameters k is 14.88987983502435 and b is -70.98450097839508\n",
      "Iteration 472, the loss is 5.201985639431301, parameters k is 14.889646337000634 and b is -70.98410572147807\n",
      "Iteration 473, the loss is 5.201969597671696, parameters k is 14.88988473621012 and b is -70.98363141317768\n",
      "Iteration 474, the loss is 5.20194852273594, parameters k is 14.889651238186405 and b is -70.98323615626067\n",
      "Iteration 475, the loss is 5.201931097598152, parameters k is 14.889417740162688 and b is -70.98284089934367\n",
      "Iteration 476, the loss is 5.201914267010967, parameters k is 14.889656139372175 and b is -70.98236659104327\n",
      "Iteration 477, the loss is 5.2018931920752145, parameters k is 14.889422641348458 and b is -70.98197133412627\n",
      "Iteration 478, the loss is 5.201876555765006, parameters k is 14.889189143324742 and b is -70.98157607720927\n",
      "Iteration 479, the loss is 5.201858936350231, parameters k is 14.889427542534229 and b is -70.98110176890887\n",
      "Iteration 480, the loss is 5.2018383108336215, parameters k is 14.889194044510512 and b is -70.98070651199187\n",
      "Iteration 481, the loss is 5.2018210496511, parameters k is 14.889210032652805 and b is -70.98027172938318\n",
      "Iteration 482, the loss is 5.201802569916487, parameters k is 14.888976534629089 and b is -70.97987647246617\n",
      "Iteration 483, the loss is 5.201783640762738, parameters k is 14.888992522771382 and b is -70.97944168985748\n",
      "Iteration 484, the loss is 5.201766351188579, parameters k is 14.889008510913674 and b is -70.97900690724879\n",
      "Iteration 485, the loss is 5.201747899845596, parameters k is 14.888775012889958 and b is -70.97861165033179\n",
      "Iteration 486, the loss is 5.2017289706918435, parameters k is 14.888791001032251 and b is -70.97817686772309\n",
      "Iteration 487, the loss is 5.201711652726057, parameters k is 14.888806989174544 and b is -70.9777420851144\n",
      "Iteration 488, the loss is 5.2016932297747065, parameters k is 14.888573491150828 and b is -70.9773468281974\n",
      "Iteration 489, the loss is 5.201674300620954, parameters k is 14.88858947929312 and b is -70.9769120455887\n",
      "Iteration 490, the loss is 5.201656954263535, parameters k is 14.888605467435413 and b is -70.97647726298001\n",
      "Iteration 491, the loss is 5.201638559703819, parameters k is 14.888371969411697 and b is -70.97608200606301\n",
      "Iteration 492, the loss is 5.201619630550068, parameters k is 14.88838795755399 and b is -70.97564722345432\n",
      "Iteration 493, the loss is 5.201602255801014, parameters k is 14.888403945696282 and b is -70.97521244084562\n",
      "Iteration 494, the loss is 5.201583889632931, parameters k is 14.888170447672566 and b is -70.97481718392862\n",
      "Iteration 495, the loss is 5.201564960479176, parameters k is 14.888186435814859 and b is -70.97438240131993\n",
      "Iteration 496, the loss is 5.2015475573384915, parameters k is 14.888202423957152 and b is -70.97394761871124\n",
      "Iteration 497, the loss is 5.20152921956204, parameters k is 14.887968925933436 and b is -70.97355236179423\n",
      "Iteration 498, the loss is 5.201510290408294, parameters k is 14.887984914075728 and b is -70.97311757918554\n",
      "Iteration 499, the loss is 5.201492858875965, parameters k is 14.888000902218021 and b is -70.97268279657685\n"
     ]
    }
   ],
   "source": [
    "#initialized parameters\n",
    "\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "iteration_num = 500 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = partial_derivative_b(y, price_use_current_parameters)\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3345cf9c50>]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3345c55110>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm,y)\n",
    "plt.scatter(X_rm,price_use_current_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一开始不能收敛，改了learning rate 和迭代次数，把learning rate改大，迭代次数改多，收敛了，可以看到绝对值损失函数受误差影响比较大，所以收敛比较慢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
